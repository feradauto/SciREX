{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2545ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sys import argv\n",
    "from typing import Dict, List\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from allennlp.common.util import import_submodules\n",
    "from allennlp.data import DataIterator, DatasetReader\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a7aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archive_folder = \"/cluster/scratch/fgonzalez/SciREX/outputs/pwc_outputs/experiment_scirex_full/main3\"\n",
    "test_file = \"/cluster/scratch/fgonzalez/test_outputs/ner_predictions.jsonl\"\n",
    "output_file = \"/cluster/scratch/fgonzalez/test_outputs/salient_mentions_predictions.jsonl\"\n",
    "cuda_device = int(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d059af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"/cluster/scratch/fgonzalez/SciREX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d99fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/cluster/scratch/fgonzalez/SciREX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f5d41f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import_submodules(\"scirex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206eac5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:36,281:INFO:Loading Model from /cluster/scratch/fgonzalez/SciREX/outputs/pwc_outputs/experiment_scirex_full/main3\n",
      "2022-03-16 18:20:36,285:INFO:loading archive file /cluster/scratch/fgonzalez/SciREX/outputs/pwc_outputs/experiment_scirex_full/main3/model.tar.gz\n",
      "2022-03-16 18:20:36,286:INFO:extracting archive file /cluster/scratch/fgonzalez/SciREX/outputs/pwc_outputs/experiment_scirex_full/main3/model.tar.gz to temp dir /scratch/209030495.tmpdir/tmpdi0xd90s\n",
      "2022-03-16 18:20:39,263:INFO:instantiating registered subclass scirex_model of <class 'allennlp.models.model.Model'>\n",
      "2022-03-16 18:20:39,264:INFO:type = default\n",
      "2022-03-16 18:20:39,264:INFO:instantiating registered subclass default of <class 'allennlp.data.vocabulary.Vocabulary'>\n",
      "2022-03-16 18:20:39,265:INFO:Loading token dictionary from /scratch/209030495.tmpdir/tmpdi0xd90s/vocabulary.\n",
      "2022-03-16 18:20:39,266:INFO:instantiating class <class 'allennlp.models.model.Model'> from params {'context_layer': {'bidirectional': True, 'hidden_size': 200, 'input_size': 768, 'type': 'lstm'}, 'display_metrics': ['validation_metric'], 'lexical_dropout': 0.2, 'loss_weights': {'n_ary_relation': '1', 'ner': '1', 'saliency': '1'}, 'modules': {'coref': {'antecedent_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2}}, 'n_ary_relation': {'antecedent_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 4840, 'num_layers': 2}, 'relation_cardinality': 2}, 'ner': {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}}, 'saliency_classifier': {'label_namespace': 'span_saliency_labels', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2}, 'n_features': 10}}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'token_embedders': {'bert': {'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'}}}, 'type': 'scirex_model'} and extras {'vocab'}\n",
      "2022-03-16 18:20:39,267:INFO:model.type = scirex_model\n",
      "2022-03-16 18:20:39,267:INFO:instantiating class <class 'scirex.models.scirex_model.ScirexModel'> from params {'context_layer': {'bidirectional': True, 'hidden_size': 200, 'input_size': 768, 'type': 'lstm'}, 'display_metrics': ['validation_metric'], 'lexical_dropout': 0.2, 'loss_weights': {'n_ary_relation': '1', 'ner': '1', 'saliency': '1'}, 'modules': {'coref': {'antecedent_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2}}, 'n_ary_relation': {'antecedent_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 4840, 'num_layers': 2}, 'relation_cardinality': 2}, 'ner': {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}}, 'saliency_classifier': {'label_namespace': 'span_saliency_labels', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2}, 'n_features': 10}}, 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'token_embedders': {'bert': {'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'}}}} and extras {'vocab'}\n",
      "2022-03-16 18:20:39,268:INFO:instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets']}, 'token_embedders': {'bert': {'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'}}} and extras {'vocab'}\n",
      "2022-03-16 18:20:39,268:INFO:model.text_field_embedder.type = basic\n",
      "2022-03-16 18:20:39,268:INFO:model.text_field_embedder.allow_unmatched_keys = True\n",
      "2022-03-16 18:20:39,269:INFO:instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz', 'requires_grad': '10,11,pooler', 'type': 'bert-pretrained-modified'} and extras {'vocab'}\n",
      "2022-03-16 18:20:39,269:INFO:model.text_field_embedder.token_embedders.bert.type = bert-pretrained-modified\n",
      "2022-03-16 18:20:39,269:INFO:instantiating class <class 'scirex.models.bert_token_embedder_modified.PretrainedBertEmbedder'> from params {'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz', 'requires_grad': '10,11,pooler'} and extras {'vocab'}\n",
      "2022-03-16 18:20:39,270:INFO:model.text_field_embedder.token_embedders.bert.pretrained_model = /cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz\n",
      "2022-03-16 18:20:39,270:INFO:model.text_field_embedder.token_embedders.bert.requires_grad = 10,11,pooler\n",
      "2022-03-16 18:20:39,270:INFO:model.text_field_embedder.token_embedders.bert.top_layer_only = False\n",
      "2022-03-16 18:20:39,271:INFO:model.text_field_embedder.token_embedders.bert.scalar_mix_parameters = None\n",
      "2022-03-16 18:20:39,272:INFO:loading archive file /cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz\n",
      "2022-03-16 18:20:39,273:INFO:extracting archive file /cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//weights.tar.gz to temp dir /scratch/209030495.tmpdir/tmpvtxq3rp0\n",
      "2022-03-16 18:20:42,102:INFO:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "2022-03-16 18:20:43,972:INFO:instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'hidden_size': 200, 'input_size': 768, 'type': 'lstm'} and extras {'vocab'}\n",
      "2022-03-16 18:20:43,973:INFO:model.context_layer.type = lstm\n",
      "2022-03-16 18:20:43,974:INFO:model.context_layer.batch_first = True\n",
      "2022-03-16 18:20:43,974:INFO:model.context_layer.stateful = False\n",
      "2022-03-16 18:20:43,974:INFO:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2022-03-16 18:20:43,975:INFO:CURRENTLY DEFINED PARAMETERS: \n",
      "2022-03-16 18:20:43,975:INFO:model.context_layer.bidirectional = True\n",
      "2022-03-16 18:20:43,975:INFO:model.context_layer.hidden_size = 200\n",
      "2022-03-16 18:20:43,976:INFO:model.context_layer.input_size = 768\n",
      "2022-03-16 18:20:43,976:INFO:model.context_layer.batch_first = True\n",
      "2022-03-16 18:20:43,987:INFO:model.modules = {'coref': {'antecedent_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2}}, 'n_ary_relation': {'antecedent_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 4840, 'num_layers': 2}, 'relation_cardinality': 2}, 'ner': {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}}, 'saliency_classifier': {'label_namespace': 'span_saliency_labels', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2}, 'n_features': 10}}\n",
      "2022-03-16 18:20:43,987:INFO:model.loss_weights = {'n_ary_relation': '1', 'ner': '1', 'saliency': '1'}\n",
      "2022-03-16 18:20:43,988:INFO:model.lexical_dropout = 0.2\n",
      "2022-03-16 18:20:43,988:INFO:model.display_metrics = ['validation_metric']\n",
      "2022-03-16 18:20:43,988:INFO:instantiating class <class 'scirex.models.ner.ner_crf_tagger.NERTagger'> from params {'exact_match': 'false', 'label_encoding': 'BIOUL', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2}} and extras {'vocab'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:43,989:INFO:instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 400, 'num_layers': 2} and extras {'vocab'}\n",
      "2022-03-16 18:20:43,989:INFO:ner.mention_feedforward.input_dim = 400\n",
      "2022-03-16 18:20:43,990:INFO:ner.mention_feedforward.num_layers = 2\n",
      "2022-03-16 18:20:43,990:INFO:ner.mention_feedforward.hidden_dims = 150\n",
      "2022-03-16 18:20:43,990:INFO:ner.mention_feedforward.activations = gelu\n",
      "2022-03-16 18:20:43,990:INFO:instantiating registered subclass gelu of <class 'allennlp.nn.activations.Activation'>\n",
      "2022-03-16 18:20:43,991:INFO:ner.mention_feedforward.dropout = 0.2\n",
      "2022-03-16 18:20:43,993:INFO:ner.label_namespace = ner_type_labels\n",
      "2022-03-16 18:20:43,993:INFO:ner.label_encoding = BIOUL\n",
      "2022-03-16 18:20:43,993:INFO:ner.exact_match = false\n",
      "2022-03-16 18:20:43,996:INFO:Initializing parameters\n",
      "2022-03-16 18:20:43,996:INFO:Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2022-03-16 18:20:43,996:INFO:   _mention_feedforward._module._linear_layers.0.bias\n",
      "2022-03-16 18:20:43,996:INFO:   _mention_feedforward._module._linear_layers.0.weight\n",
      "2022-03-16 18:20:43,997:INFO:   _mention_feedforward._module._linear_layers.1.bias\n",
      "2022-03-16 18:20:43,997:INFO:   _mention_feedforward._module._linear_layers.1.weight\n",
      "2022-03-16 18:20:43,997:INFO:   _ner_crf._constraint_mask\n",
      "2022-03-16 18:20:43,998:INFO:   _ner_crf.transitions\n",
      "2022-03-16 18:20:43,998:INFO:   _ner_scorer._module.bias\n",
      "2022-03-16 18:20:43,998:INFO:   _ner_scorer._module.weight\n",
      "2022-03-16 18:20:43,998:INFO:instantiating class <class 'scirex.models.span_classifiers.span_classifier.SpanClassifier'> from params {'label_namespace': 'span_saliency_labels', 'mention_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2}, 'n_features': 10} and extras {'vocab'}\n",
      "2022-03-16 18:20:43,999:INFO:instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 1210, 'num_layers': 2} and extras {'vocab'}\n",
      "2022-03-16 18:20:43,999:INFO:saliency_classifier.mention_feedforward.input_dim = 1210\n",
      "2022-03-16 18:20:43,999:INFO:saliency_classifier.mention_feedforward.num_layers = 2\n",
      "2022-03-16 18:20:44,000:INFO:saliency_classifier.mention_feedforward.hidden_dims = 150\n",
      "2022-03-16 18:20:44,000:INFO:saliency_classifier.mention_feedforward.activations = gelu\n",
      "2022-03-16 18:20:44,000:INFO:instantiating registered subclass gelu of <class 'allennlp.nn.activations.Activation'>\n",
      "2022-03-16 18:20:44,001:INFO:saliency_classifier.mention_feedforward.dropout = 0.2\n",
      "2022-03-16 18:20:44,003:INFO:saliency_classifier.label_namespace = span_saliency_labels\n",
      "2022-03-16 18:20:44,003:INFO:saliency_classifier.n_features = 10\n",
      "2022-03-16 18:20:44,004:INFO:Initializing parameters\n",
      "2022-03-16 18:20:44,004:INFO:Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2022-03-16 18:20:44,005:INFO:   _mention_feedforward._module._linear_layers.0.bias\n",
      "2022-03-16 18:20:44,005:INFO:   _mention_feedforward._module._linear_layers.0.weight\n",
      "2022-03-16 18:20:44,005:INFO:   _mention_feedforward._module._linear_layers.1.bias\n",
      "2022-03-16 18:20:44,005:INFO:   _mention_feedforward._module._linear_layers.1.weight\n",
      "2022-03-16 18:20:44,006:INFO:   _ner_scorer._module.bias\n",
      "2022-03-16 18:20:44,006:INFO:   _ner_scorer._module.weight\n",
      "2022-03-16 18:20:44,006:INFO:instantiating class <class 'scirex.models.relations.entity_relation.RelationExtractor'> from params {'antecedent_feedforward': {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 4840, 'num_layers': 2}, 'relation_cardinality': 2} and extras {'vocab'}\n",
      "2022-03-16 18:20:44,007:INFO:instantiating class <class 'allennlp.modules.feedforward.FeedForward'> from params {'activations': 'gelu', 'dropout': 0.2, 'hidden_dims': 150, 'input_dim': 4840, 'num_layers': 2} and extras {'vocab'}\n",
      "2022-03-16 18:20:44,007:INFO:n_ary_relation.antecedent_feedforward.input_dim = 4840\n",
      "2022-03-16 18:20:44,007:INFO:n_ary_relation.antecedent_feedforward.num_layers = 2\n",
      "2022-03-16 18:20:44,008:INFO:n_ary_relation.antecedent_feedforward.hidden_dims = 150\n",
      "2022-03-16 18:20:44,008:INFO:n_ary_relation.antecedent_feedforward.activations = gelu\n",
      "2022-03-16 18:20:44,008:INFO:instantiating registered subclass gelu of <class 'allennlp.nn.activations.Activation'>\n",
      "2022-03-16 18:20:44,008:INFO:n_ary_relation.antecedent_feedforward.dropout = 0.2\n",
      "2022-03-16 18:20:44,014:INFO:n_ary_relation.relation_cardinality = 2\n",
      "2022-03-16 18:20:44,015:INFO:Initializing parameters\n",
      "2022-03-16 18:20:44,015:INFO:Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2022-03-16 18:20:44,015:INFO:   _antecedent_feedforward._module._linear_layers.0.bias\n",
      "2022-03-16 18:20:44,016:INFO:   _antecedent_feedforward._module._linear_layers.0.weight\n",
      "2022-03-16 18:20:44,016:INFO:   _antecedent_feedforward._module._linear_layers.1.bias\n",
      "2022-03-16 18:20:44,016:INFO:   _antecedent_feedforward._module._linear_layers.1.weight\n",
      "2022-03-16 18:20:44,016:INFO:   _antecedent_scorer._module.bias\n",
      "2022-03-16 18:20:44,017:INFO:   _antecedent_scorer._module.weight\n",
      "2022-03-16 18:20:44,017:INFO:   _bias_vectors\n",
      "2022-03-16 18:20:44,018:INFO:Initializing parameters\n",
      "2022-03-16 18:20:44,019:INFO:Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2022-03-16 18:20:44,020:INFO:   _attentive_span_extractor._global_attention._module.bias\n",
      "2022-03-16 18:20:44,020:INFO:   _attentive_span_extractor._global_attention._module.weight\n",
      "2022-03-16 18:20:44,020:INFO:   _cluster_n_ary_relation._antecedent_feedforward._module._linear_layers.0.bias\n",
      "2022-03-16 18:20:44,020:INFO:   _cluster_n_ary_relation._antecedent_feedforward._module._linear_layers.0.weight\n",
      "2022-03-16 18:20:44,021:INFO:   _cluster_n_ary_relation._antecedent_feedforward._module._linear_layers.1.bias\n",
      "2022-03-16 18:20:44,021:INFO:   _cluster_n_ary_relation._antecedent_feedforward._module._linear_layers.1.weight\n",
      "2022-03-16 18:20:44,021:INFO:   _cluster_n_ary_relation._antecedent_scorer._module.bias\n",
      "2022-03-16 18:20:44,022:INFO:   _cluster_n_ary_relation._antecedent_scorer._module.weight\n",
      "2022-03-16 18:20:44,022:INFO:   _cluster_n_ary_relation._bias_vectors\n",
      "2022-03-16 18:20:44,022:INFO:   _context_layer._module.bias_hh_l0\n",
      "2022-03-16 18:20:44,022:INFO:   _context_layer._module.bias_hh_l0_reverse\n",
      "2022-03-16 18:20:44,023:INFO:   _context_layer._module.bias_ih_l0\n",
      "2022-03-16 18:20:44,023:INFO:   _context_layer._module.bias_ih_l0_reverse\n",
      "2022-03-16 18:20:44,023:INFO:   _context_layer._module.weight_hh_l0\n",
      "2022-03-16 18:20:44,023:INFO:   _context_layer._module.weight_hh_l0_reverse\n",
      "2022-03-16 18:20:44,024:INFO:   _context_layer._module.weight_ih_l0\n",
      "2022-03-16 18:20:44,024:INFO:   _context_layer._module.weight_ih_l0_reverse\n",
      "2022-03-16 18:20:44,024:INFO:   _ner._mention_feedforward._module._linear_layers.0.bias\n",
      "2022-03-16 18:20:44,025:INFO:   _ner._mention_feedforward._module._linear_layers.0.weight\n",
      "2022-03-16 18:20:44,025:INFO:   _ner._mention_feedforward._module._linear_layers.1.bias\n",
      "2022-03-16 18:20:44,025:INFO:   _ner._mention_feedforward._module._linear_layers.1.weight\n",
      "2022-03-16 18:20:44,025:INFO:   _ner._ner_crf._constraint_mask\n",
      "2022-03-16 18:20:44,026:INFO:   _ner._ner_crf.transitions\n",
      "2022-03-16 18:20:44,026:INFO:   _ner._ner_scorer._module.bias\n",
      "2022-03-16 18:20:44,026:INFO:   _ner._ner_scorer._module.weight\n",
      "2022-03-16 18:20:44,026:INFO:   _saliency_classifier._mention_feedforward._module._linear_layers.0.bias\n",
      "2022-03-16 18:20:44,027:INFO:   _saliency_classifier._mention_feedforward._module._linear_layers.0.weight\n",
      "2022-03-16 18:20:44,027:INFO:   _saliency_classifier._mention_feedforward._module._linear_layers.1.bias\n",
      "2022-03-16 18:20:44,027:INFO:   _saliency_classifier._mention_feedforward._module._linear_layers.1.weight\n",
      "2022-03-16 18:20:44,028:INFO:   _saliency_classifier._ner_scorer._module.bias\n",
      "2022-03-16 18:20:44,028:INFO:   _saliency_classifier._ner_scorer._module.weight\n",
      "2022-03-16 18:20:44,028:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.gamma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:44,028:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
      "2022-03-16 18:20:44,029:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
      "2022-03-16 18:20:44,029:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
      "2022-03-16 18:20:44,029:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
      "2022-03-16 18:20:44,029:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
      "2022-03-16 18:20:44,030:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
      "2022-03-16 18:20:44,030:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
      "2022-03-16 18:20:44,030:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
      "2022-03-16 18:20:44,031:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
      "2022-03-16 18:20:44,031:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
      "2022-03-16 18:20:44,031:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
      "2022-03-16 18:20:44,031:INFO:   _text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
      "2022-03-16 18:20:44,032:INFO:   _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
      "2022-03-16 18:20:44,032:INFO:   _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
      "2022-03-16 18:20:44,032:INFO:   _text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
      "2022-03-16 18:20:44,032:INFO:   _text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
      "2022-03-16 18:20:44,033:INFO:   _text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
      "2022-03-16 18:20:44,033:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,033:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,033:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,034:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,034:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
      "2022-03-16 18:20:44,034:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
      "2022-03-16 18:20:44,035:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
      "2022-03-16 18:20:44,035:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
      "2022-03-16 18:20:44,035:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
      "2022-03-16 18:20:44,035:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
      "2022-03-16 18:20:44,036:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,036:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,036:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,036:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,037:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
      "2022-03-16 18:20:44,037:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
      "2022-03-16 18:20:44,037:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,038:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,038:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,038:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,038:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
      "2022-03-16 18:20:44,039:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
      "2022-03-16 18:20:44,039:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
      "2022-03-16 18:20:44,039:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
      "2022-03-16 18:20:44,039:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
      "2022-03-16 18:20:44,040:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
      "2022-03-16 18:20:44,040:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,040:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,047:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,047:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,047:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
      "2022-03-16 18:20:44,048:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
      "2022-03-16 18:20:44,048:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,048:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,049:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,049:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,049:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
      "2022-03-16 18:20:44,049:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
      "2022-03-16 18:20:44,050:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
      "2022-03-16 18:20:44,050:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
      "2022-03-16 18:20:44,050:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
      "2022-03-16 18:20:44,050:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
      "2022-03-16 18:20:44,051:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,051:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,051:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,052:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,052:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
      "2022-03-16 18:20:44,052:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
      "2022-03-16 18:20:44,052:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,053:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,053:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:44,053:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,053:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
      "2022-03-16 18:20:44,054:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
      "2022-03-16 18:20:44,054:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
      "2022-03-16 18:20:44,054:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
      "2022-03-16 18:20:44,055:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
      "2022-03-16 18:20:44,055:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
      "2022-03-16 18:20:44,055:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,055:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,056:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,056:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,056:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
      "2022-03-16 18:20:44,056:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
      "2022-03-16 18:20:44,057:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,057:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,057:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,057:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,058:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
      "2022-03-16 18:20:44,058:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
      "2022-03-16 18:20:44,058:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
      "2022-03-16 18:20:44,059:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
      "2022-03-16 18:20:44,059:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
      "2022-03-16 18:20:44,059:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
      "2022-03-16 18:20:44,059:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,060:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,060:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,060:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,060:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
      "2022-03-16 18:20:44,061:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
      "2022-03-16 18:20:44,061:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,061:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,062:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,062:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,062:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
      "2022-03-16 18:20:44,062:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
      "2022-03-16 18:20:44,063:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
      "2022-03-16 18:20:44,063:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
      "2022-03-16 18:20:44,063:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
      "2022-03-16 18:20:44,063:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
      "2022-03-16 18:20:44,064:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,064:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,064:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,065:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,065:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
      "2022-03-16 18:20:44,065:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
      "2022-03-16 18:20:44,065:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,066:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,066:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,066:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,066:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
      "2022-03-16 18:20:44,067:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
      "2022-03-16 18:20:44,067:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
      "2022-03-16 18:20:44,067:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
      "2022-03-16 18:20:44,067:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
      "2022-03-16 18:20:44,068:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
      "2022-03-16 18:20:44,068:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,068:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,069:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,069:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,069:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
      "2022-03-16 18:20:44,069:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
      "2022-03-16 18:20:44,070:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,070:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,070:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,070:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:44,071:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
      "2022-03-16 18:20:44,071:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
      "2022-03-16 18:20:44,071:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
      "2022-03-16 18:20:44,072:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
      "2022-03-16 18:20:44,072:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
      "2022-03-16 18:20:44,072:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
      "2022-03-16 18:20:44,072:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,073:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,073:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,073:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,073:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
      "2022-03-16 18:20:44,074:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
      "2022-03-16 18:20:44,074:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,074:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,074:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,075:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,075:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
      "2022-03-16 18:20:44,075:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
      "2022-03-16 18:20:44,076:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
      "2022-03-16 18:20:44,076:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
      "2022-03-16 18:20:44,076:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
      "2022-03-16 18:20:44,076:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
      "2022-03-16 18:20:44,077:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,077:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,077:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,077:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,078:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
      "2022-03-16 18:20:44,078:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
      "2022-03-16 18:20:44,078:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,079:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,079:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,079:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,079:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
      "2022-03-16 18:20:44,080:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
      "2022-03-16 18:20:44,080:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
      "2022-03-16 18:20:44,080:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
      "2022-03-16 18:20:44,080:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
      "2022-03-16 18:20:44,081:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
      "2022-03-16 18:20:44,081:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,081:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,082:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,082:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,082:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
      "2022-03-16 18:20:44,082:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
      "2022-03-16 18:20:44,083:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,083:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,083:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,083:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,084:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
      "2022-03-16 18:20:44,084:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
      "2022-03-16 18:20:44,084:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
      "2022-03-16 18:20:44,085:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
      "2022-03-16 18:20:44,085:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
      "2022-03-16 18:20:44,085:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
      "2022-03-16 18:20:44,085:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,086:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,086:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,086:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,086:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
      "2022-03-16 18:20:44,087:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
      "2022-03-16 18:20:44,087:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,087:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,088:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "2022-03-16 18:20:44,088:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "2022-03-16 18:20:44,088:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
      "2022-03-16 18:20:44,088:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:44,089:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
      "2022-03-16 18:20:44,089:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
      "2022-03-16 18:20:44,089:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
      "2022-03-16 18:20:44,089:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
      "2022-03-16 18:20:44,090:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "2022-03-16 18:20:44,090:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "2022-03-16 18:20:44,090:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2022-03-16 18:20:44,091:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2022-03-16 18:20:44,091:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
      "2022-03-16 18:20:44,091:INFO:   _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
      "2022-03-16 18:20:44,091:INFO:   _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
      "2022-03-16 18:20:44,092:INFO:   _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Method', 2: 'L-Method', 3: 'I-Method', 4: 'U-Method', 5: 'B-Task', 6: 'L-Task', 7: 'I-Task', 8: 'B-Metric', 9: 'L-Metric', 10: 'U-Task', 11: 'U-Metric', 12: 'B-Material', 13: 'L-Material', 14: 'I-Metric', 15: 'I-Material', 16: 'U-Material'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:47,274:INFO:instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'token_indexers': {'bert': {'do_lowercase': 'true', 'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt', 'truncate_long_sequences': False, 'type': 'bert-pretrained', 'use_starting_offsets': True}}, 'type': 'scirex_full_reader'} and extras set()\n",
      "2022-03-16 18:20:47,274:INFO:dataset_reader.type = scirex_full_reader\n",
      "2022-03-16 18:20:47,275:INFO:instantiating class <class 'scirex.data.dataset_readers.scirex_full_reader.ScirexFullReader'> from params {'token_indexers': {'bert': {'do_lowercase': 'true', 'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt', 'truncate_long_sequences': False, 'type': 'bert-pretrained', 'use_starting_offsets': True}}} and extras set()\n",
      "2022-03-16 18:20:47,275:INFO:instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'do_lowercase': 'true', 'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt', 'truncate_long_sequences': False, 'type': 'bert-pretrained', 'use_starting_offsets': True} and extras set()\n",
      "2022-03-16 18:20:47,276:INFO:dataset_reader.token_indexers.bert.type = bert-pretrained\n",
      "2022-03-16 18:20:47,276:INFO:instantiating class <class 'allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer'> from params {'do_lowercase': 'true', 'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt', 'truncate_long_sequences': False, 'use_starting_offsets': True} and extras set()\n",
      "2022-03-16 18:20:47,277:INFO:dataset_reader.token_indexers.bert.pretrained_model = /cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt\n",
      "2022-03-16 18:20:47,277:INFO:dataset_reader.token_indexers.bert.use_starting_offsets = True\n",
      "2022-03-16 18:20:47,277:INFO:dataset_reader.token_indexers.bert.do_lowercase = true\n",
      "2022-03-16 18:20:47,277:INFO:dataset_reader.token_indexers.bert.never_lowercase = None\n",
      "2022-03-16 18:20:47,278:INFO:dataset_reader.token_indexers.bert.max_pieces = 512\n",
      "2022-03-16 18:20:47,278:INFO:dataset_reader.token_indexers.bert.truncate_long_sequences = False\n",
      "2022-03-16 18:20:47,279:INFO:loading vocabulary file /cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt\n",
      "2022-03-16 18:20:47,312:INFO:dataset_reader.max_paragraph_length = 300\n",
      "2022-03-16 18:20:47,313:INFO:dataset_reader.lazy = False\n",
      "2022-03-16 18:20:47,313:INFO:dataset_reader.to_scirex_converter = False\n",
      "2096it [00:01, 1112.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(\"Loading Model from %s\", archive_folder)\n",
    "archive_file = os.path.join(archive_folder, \"model.tar.gz\")\n",
    "archive = load_archive(archive_file, cuda_device)\n",
    "model = archive.model\n",
    "model.eval()\n",
    "\n",
    "saliency_threshold = json.load(open(archive_folder + '/metrics.json'))['best_validation__span_threshold']\n",
    "\n",
    "model.prediction_mode = True\n",
    "config = archive.config.duplicate()\n",
    "dataset_reader_params = config[\"dataset_reader\"]\n",
    "dataset_reader = DatasetReader.from_params(dataset_reader_params)\n",
    "dataset_reader.prediction_mode = True\n",
    "instances = dataset_reader.read(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d833ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance in instances :\n",
    "    batch = Batch([instance])\n",
    "    batch.index_instances(model.vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c20c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 18:20:51,096:INFO:instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 10, 'type': 'ie_batch'} and extras set()\n",
      "2022-03-16 18:20:51,097:INFO:validation_iterator.type = ie_batch\n",
      "2022-03-16 18:20:51,098:INFO:instantiating class <class 'scirex.data.iterators.batch_iterator.BatchIterator'> from params {'batch_size': 10} and extras set()\n",
      "2022-03-16 18:20:51,098:INFO:validation_iterator.batch_size = 10\n",
      "2022-03-16 18:20:51,098:INFO:validation_iterator.instances_per_epoch = None\n",
      "2022-03-16 18:20:51,099:INFO:validation_iterator.max_instances_in_memory = None\n",
      "2022-03-16 18:20:51,099:INFO:validation_iterator.cache_instances = False\n",
      "2022-03-16 18:20:51,099:INFO:validation_iterator.track_epoch = False\n",
      "2022-03-16 18:20:51,100:INFO:validation_iterator.maximum_samples_per_batch = None\n"
     ]
    }
   ],
   "source": [
    "data_iterator = DataIterator.from_params(config[\"validation_iterator\"])\n",
    "iterator = data_iterator(instances, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2e39440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <allennlp.data.fields.text_field.TextField at 0x2adc4ad34510>,\n",
       " 'ner_type_labels': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x2adc4ad345d0>,\n",
       " 'metadata': <allennlp.data.fields.metadata_field.MetadataField at 0x2adc4ad34550>,\n",
       " 'spans': <allennlp.data.fields.list_field.ListField at 0x2adc4ad34810>,\n",
       " 'span_cluster_labels': <allennlp.data.fields.list_field.ListField at 0x2adc4ad346d0>,\n",
       " 'span_saliency_labels': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x2adc4ad34610>,\n",
       " 'span_type_labels': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x2adc4ad34650>,\n",
       " 'span_features': <allennlp.data.fields.list_field.ListField at 0x2adc4ad34850>}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37b985e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ListField.as_tensor of <allennlp.data.fields.list_field.ListField object at 0x2adc4ad34810>>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.get('spans').as_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ebf979ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:00, 218.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [00:00, 214.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "102it [00:00, 211.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n",
      "tensor([[[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]],\n",
      "\n",
      "        [[-1, -1]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(iterator):\n",
    "    print(batch[\"spans\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e04bcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1, -1]],\n",
       "\n",
       "        [[-1, -1]],\n",
       "\n",
       "        [[-1, -1]],\n",
       "\n",
       "        [[-1, -1]],\n",
       "\n",
       "        [[-1, -1]]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"spans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d62250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_saliency(self, batch, saliency_threshold):\n",
    "    output_embedding = self.embedding_forward(text=batch[\"text\"])\n",
    "    output_span_embedding = self.span_embeddings_forward(\n",
    "        output_embedding=output_embedding,\n",
    "        spans=batch[\"spans\"],\n",
    "        span_type_labels=batch[\"span_type_labels\"],\n",
    "        span_features=batch[\"span_features\"],\n",
    "        metadata=batch[\"metadata\"],\n",
    "    )\n",
    "\n",
    "    output_saliency = self.saliency_forward(\n",
    "        output_span_embedding=output_span_embedding,\n",
    "        metadata=batch[\"metadata\"],\n",
    "        span_saliency_labels=batch[\"span_saliency_labels\"],\n",
    "        span_cluster_labels=batch[\"span_cluster_labels\"],\n",
    "        saliency_threshold=saliency_threshold,\n",
    "    )\n",
    "\n",
    "    return self._saliency_classifier.decode(output_saliency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c1376ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embedding = model.embedding_forward(text=batch[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f7701fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=batch[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3826d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4030e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = model._lexical_dropout(model._text_field_embedder(text))\n",
    "text_mask = util.get_text_field_mask(text)\n",
    "sentence_lengths = text_mask.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b3fef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_text_embeddings = text_embeddings.view(-1, text_embeddings.size(-1))\n",
    "flat_text_mask = text_mask.view(-1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "646c8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text_embeddings = flat_text_embeddings[flat_text_mask.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3770cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "20287697",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids: List[str]= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ff9eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_contextualized_embeddings = model._context_layer(\n",
    "        filtered_text_embeddings.unsqueeze(0),\n",
    "        torch.ones((1, filtered_text_embeddings.size(0)), device=filtered_text_embeddings.device).byte(),\n",
    "    ).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d314459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7595,  0.1845, -0.5993,  ..., -0.0827, -0.9913, -0.5960],\n",
       "        [ 0.4092, -0.5473,  0.1391,  ..., -0.2535, -1.3709, -0.8746],\n",
       "        [ 0.0253,  0.2964, -0.2465,  ..., -0.5693, -0.5737, -0.0314],\n",
       "        ...,\n",
       "        [-0.8255,  0.2050, -0.5399,  ..., -1.0783, -1.0213,  0.0763],\n",
       "        [-0.0963,  0.2643, -0.3277,  ..., -0.3970, -0.4930, -0.6844],\n",
       "        [-0.7601, -0.5484, -0.5534,  ..., -0.7049, -1.4994,  0.5445]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66b52af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.6825e-04, -1.2295e-02,  1.1463e-02,  ...,  2.6295e-03,\n",
       "         -1.7944e-03,  7.7497e-03],\n",
       "        [-1.7461e-03, -6.1071e-01,  1.5967e-02,  ...,  3.0832e-02,\n",
       "          2.8066e-03,  6.0617e-01],\n",
       "        [ 1.4560e-01, -5.8072e-04,  3.9946e-07,  ..., -6.8800e-04,\n",
       "          7.6671e-04,  3.3265e-01],\n",
       "        ...,\n",
       "        [-4.8250e-03, -8.5825e-01,  1.2302e-01,  ...,  1.8657e-02,\n",
       "         -1.3855e-04,  5.2764e-01],\n",
       "        [-1.3995e-02, -9.2483e-01,  3.1442e-01,  ...,  3.5080e-01,\n",
       "          1.0244e-02,  5.9603e-01],\n",
       "        [-2.0613e-01, -4.5218e-01,  5.9343e-02,  ...,  2.7646e-01,\n",
       "          4.2228e-04,  9.3150e-02]], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_contextualized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526aa78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_forward(self, text):\n",
    "    # Shape: (batch_size, max_sentence_length, embedding_size)\n",
    "    \n",
    "\n",
    "    # Shape: (total_sentence_length, encoding_dim)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    flat_contextualized_embeddings = torch.zeros(\n",
    "        (flat_text_embeddings.size(0), filtered_contextualized_embeddings.size(1)),\n",
    "        device=filtered_text_embeddings.device,\n",
    "    )\n",
    "    flat_contextualized_embeddings.masked_scatter_(\n",
    "        flat_text_mask.unsqueeze(-1).bool(), filtered_contextualized_embeddings\n",
    "    )\n",
    "\n",
    "    # Shape: (batch_size, max_sentence_length, embedding_size)\n",
    "    contextualized_embeddings = flat_contextualized_embeddings.reshape(\n",
    "        (text_embeddings.size(0), text_embeddings.size(1), flat_contextualized_embeddings.size(-1))\n",
    "    )\n",
    "\n",
    "    output_embedding = {\n",
    "        \"contextualised\": contextualized_embeddings,\n",
    "        \"text\": text_embeddings,\n",
    "        \"mask\": text_mask,\n",
    "        \"lengths\": sentence_lengths,\n",
    "    }\n",
    "    return output_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b21a7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_span_embedding = model.span_embeddings_forward(\n",
    "        output_embedding=output_embedding,\n",
    "        spans=batch[\"spans\"],\n",
    "        span_type_labels=batch[\"span_type_labels\"],\n",
    "        span_features=batch[\"span_features\"],\n",
    "        metadata=batch[\"metadata\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "341fd291",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_span_embeddings = {\"valid\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8add13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_embedding=output_embedding\n",
    "spans=batch[\"spans\"]\n",
    "span_type_labels=batch[\"span_type_labels\"]\n",
    "span_features=batch[\"span_features\"]\n",
    "metadata=batch[\"metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0b63df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f171bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_mask, spans, span_embeddings = model.extract_span_embeddings(\n",
    "                output_embedding[\"contextualised\"], spans\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7918510b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1, -1]],\n",
       "\n",
       "        [[-1, -1]],\n",
       "\n",
       "        [[-1, -1]],\n",
       "\n",
       "        [[-1, -1]],\n",
       "\n",
       "        [[-1, -1]]], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02410602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28e2be54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'bert': tensor([[  102, 13271,   862,  ...,  9996,   205,   103],\n",
       "          [  102, 13271,   862,  ...,     0,     0,     0],\n",
       "          [  102,  1155,   862,  ...,     0,     0,     0],\n",
       "          [  102,  1155,   862,  ...,     0,     0,     0],\n",
       "          [  102, 29098,   862,  ...,     0,     0,     0]], device='cuda:0'),\n",
       "  'bert-offsets': tensor([[  1,   2,   3,  ..., 303, 304, 305],\n",
       "          [  1,   2,   3,  ...,   0,   0,   0],\n",
       "          [  1,   2,   3,  ...,   0,   0,   0],\n",
       "          [  1,   2,   3,  ...,   0,   0,   0],\n",
       "          [  1,   2,   3,  ...,   0,   0,   0]], device='cuda:0'),\n",
       "  'bert-type-ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
       "  'mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')},\n",
       " 'ner_type_labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'metadata': [{'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "   'paragraph_num': 20,\n",
       "   'paragraph': ['subsection',\n",
       "    ':',\n",
       "    'Learned',\n",
       "    'Representations',\n",
       "    'of',\n",
       "    'Phrases',\n",
       "    'and',\n",
       "    'Sentences',\n",
       "    'Using',\n",
       "    'cosine',\n",
       "    'similarity',\n",
       "    'between',\n",
       "    'their',\n",
       "    'representations',\n",
       "    'produced',\n",
       "    'by',\n",
       "    'the',\n",
       "    'NTI',\n",
       "    '-',\n",
       "    'SLSTM',\n",
       "    'model',\n",
       "    ',',\n",
       "    'we',\n",
       "    'show',\n",
       "    'that',\n",
       "    'NTI',\n",
       "    'is',\n",
       "    'able',\n",
       "    'to',\n",
       "    'capture',\n",
       "    'paraphrases',\n",
       "    'on',\n",
       "    'SNLI',\n",
       "    'test',\n",
       "    'data',\n",
       "    '.',\n",
       "    'As',\n",
       "    'shown',\n",
       "    'in',\n",
       "    'Table',\n",
       "    '[',\n",
       "    'reference',\n",
       "    ']',\n",
       "    ',',\n",
       "    'NTI',\n",
       "    'seems',\n",
       "    'to',\n",
       "    'distinguish',\n",
       "    'plural',\n",
       "    'from',\n",
       "    'singular',\n",
       "    'forms',\n",
       "    '(',\n",
       "    'similar',\n",
       "    'phrases',\n",
       "    'to',\n",
       "    '”',\n",
       "    'a',\n",
       "    'person',\n",
       "    '”',\n",
       "    ')',\n",
       "    '.',\n",
       "    'In',\n",
       "    'addition',\n",
       "    ',',\n",
       "    'NTI',\n",
       "    'captures',\n",
       "    'non',\n",
       "    '-',\n",
       "    'surface',\n",
       "    'knowledge',\n",
       "    '.',\n",
       "    'For',\n",
       "    'example',\n",
       "    ',',\n",
       "    'the',\n",
       "    'phrases',\n",
       "    'similar',\n",
       "    'to',\n",
       "    '”',\n",
       "    'park',\n",
       "    'for',\n",
       "    'fun',\n",
       "    '”',\n",
       "    'tend',\n",
       "    'to',\n",
       "    'align',\n",
       "    'to',\n",
       "    'the',\n",
       "    'semantic',\n",
       "    'content',\n",
       "    'of',\n",
       "    'fun',\n",
       "    'and',\n",
       "    'park',\n",
       "    ',',\n",
       "    'including',\n",
       "    '”',\n",
       "    'people',\n",
       "    'play',\n",
       "    'frisbee',\n",
       "    'outdoors',\n",
       "    '”',\n",
       "    '.',\n",
       "    'The',\n",
       "    'NTI',\n",
       "    'model',\n",
       "    'was',\n",
       "    'able',\n",
       "    'to',\n",
       "    'relate',\n",
       "    '”',\n",
       "    'Santa',\n",
       "    'Claus',\n",
       "    '”',\n",
       "    'to',\n",
       "    'christmas',\n",
       "    'and',\n",
       "    'snow',\n",
       "    '.',\n",
       "    'Interestingly',\n",
       "    ',',\n",
       "    'the',\n",
       "    'learned',\n",
       "    'representations',\n",
       "    'were',\n",
       "    'also',\n",
       "    'able',\n",
       "    'to',\n",
       "    'connect',\n",
       "    'implicit',\n",
       "    'semantics',\n",
       "    '.',\n",
       "    'For',\n",
       "    'example',\n",
       "    ',',\n",
       "    'NTI',\n",
       "    'found',\n",
       "    'that',\n",
       "    '”',\n",
       "    'sad',\n",
       "    ',',\n",
       "    'depressed',\n",
       "    ',',\n",
       "    'and',\n",
       "    'hatred',\n",
       "    '”',\n",
       "    'is',\n",
       "    'close',\n",
       "    'to',\n",
       "    'the',\n",
       "    'phrases',\n",
       "    'like',\n",
       "    '”',\n",
       "    'an',\n",
       "    'Obama',\n",
       "    'supporter',\n",
       "    'is',\n",
       "    'upset',\n",
       "    '”',\n",
       "    '.',\n",
       "    'Overall',\n",
       "    'the',\n",
       "    'NTI',\n",
       "    'model',\n",
       "    'is',\n",
       "    'robust',\n",
       "    'to',\n",
       "    'the',\n",
       "    'length',\n",
       "    'of',\n",
       "    'the',\n",
       "    'phrases',\n",
       "    'being',\n",
       "    'matched',\n",
       "    '.',\n",
       "    'Given',\n",
       "    'a',\n",
       "    'short',\n",
       "    'phrase',\n",
       "    ',',\n",
       "    'NTI',\n",
       "    'can',\n",
       "    'retrieve',\n",
       "    'longer',\n",
       "    'yet',\n",
       "    'semantically',\n",
       "    'coherent',\n",
       "    'sequences',\n",
       "    'from',\n",
       "    'the',\n",
       "    'SNLI',\n",
       "    'test',\n",
       "    'set',\n",
       "    '.',\n",
       "    'In',\n",
       "    'Table',\n",
       "    '[',\n",
       "    'reference',\n",
       "    ']',\n",
       "    ',',\n",
       "    'we',\n",
       "    'show',\n",
       "    'nearest',\n",
       "    '-',\n",
       "    'neighbor',\n",
       "    'sentences',\n",
       "    'from',\n",
       "    'SNLI',\n",
       "    'test',\n",
       "    'set',\n",
       "    '.',\n",
       "    'Note',\n",
       "    'that',\n",
       "    'the',\n",
       "    'sentences',\n",
       "    'listed',\n",
       "    'in',\n",
       "    'the',\n",
       "    'first',\n",
       "    'two',\n",
       "    'columns',\n",
       "    'sound',\n",
       "    'semantically',\n",
       "    'coherent',\n",
       "    'but',\n",
       "    'not',\n",
       "    'the',\n",
       "    'ones',\n",
       "    'in',\n",
       "    'the',\n",
       "    'last',\n",
       "    'column',\n",
       "    '.',\n",
       "    'The',\n",
       "    'query',\n",
       "    'sentence',\n",
       "    '”',\n",
       "    'A',\n",
       "    'dog',\n",
       "    'sells',\n",
       "    'a',\n",
       "    'women',\n",
       "    'a',\n",
       "    'hat',\n",
       "    '”',\n",
       "    'does',\n",
       "    'not',\n",
       "    'actually',\n",
       "    'represent',\n",
       "    'a',\n",
       "    'common',\n",
       "    '-',\n",
       "    'sense',\n",
       "    'knowledge',\n",
       "    'and',\n",
       "    'this',\n",
       "    'sentence',\n",
       "    'now',\n",
       "    'seem',\n",
       "    'to',\n",
       "    'confuse',\n",
       "    'the',\n",
       "    'NTI',\n",
       "    'model',\n",
       "    '.',\n",
       "    'As',\n",
       "    'a',\n",
       "    'result',\n",
       "    ',',\n",
       "    'the',\n",
       "    'retrieved',\n",
       "    'sentence',\n",
       "    'are',\n",
       "    'arbitrary',\n",
       "    'and',\n",
       "    'not',\n",
       "    'coherent',\n",
       "    '.'],\n",
       "   'start_pos_in_doc': 4259,\n",
       "   'end_pos_in_doc': 4538,\n",
       "   'ner_dict': {},\n",
       "   'sentence_indices': [(4259, 4267),\n",
       "    (4267, 4295),\n",
       "    (4295, 4321),\n",
       "    (4321, 4331),\n",
       "    (4331, 4363),\n",
       "    (4363, 4379),\n",
       "    (4379, 4392),\n",
       "    (4392, 4420),\n",
       "    (4420, 4435),\n",
       "    (4435, 4454),\n",
       "    (4454, 4471),\n",
       "    (4471, 4493),\n",
       "    (4493, 4525),\n",
       "    (4525, 4538)],\n",
       "   'document_metadata': {'cluster_name_to_id': {},\n",
       "    'span_to_cluster_ids': {},\n",
       "    'relation_to_cluster_ids': {},\n",
       "    'type_to_cluster_ids': {},\n",
       "    'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "    'doc_length': 5013,\n",
       "    'entities_to_features_map': {}},\n",
       "   'num_spans': 0,\n",
       "   'span_idx': (0, 0)},\n",
       "  {'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "   'paragraph_num': 21,\n",
       "   'paragraph': ['subsection',\n",
       "    ':',\n",
       "    'Effects',\n",
       "    'of',\n",
       "    'Padding',\n",
       "    'Size',\n",
       "    'We',\n",
       "    'introduced',\n",
       "    'a',\n",
       "    'special',\n",
       "    'padding',\n",
       "    'character',\n",
       "    'in',\n",
       "    'order',\n",
       "    'to',\n",
       "    'construct',\n",
       "    'full',\n",
       "    'binary',\n",
       "    'tree',\n",
       "    '.',\n",
       "    'Does',\n",
       "    'this',\n",
       "    'padding',\n",
       "    'character',\n",
       "    'influence',\n",
       "    'the',\n",
       "    'performance',\n",
       "    'of',\n",
       "    'the',\n",
       "    'NTI',\n",
       "    'models',\n",
       "    '?',\n",
       "    'In',\n",
       "    'Figure',\n",
       "    '[',\n",
       "    'reference',\n",
       "    ']',\n",
       "    ',',\n",
       "    'we',\n",
       "    'show',\n",
       "    'relationship',\n",
       "    'between',\n",
       "    'the',\n",
       "    'padding',\n",
       "    'size',\n",
       "    'and',\n",
       "    'the',\n",
       "    'accuracy',\n",
       "    'on',\n",
       "    'Stanford',\n",
       "    'sentiment',\n",
       "    'analysis',\n",
       "    'data',\n",
       "    '.',\n",
       "    'Each',\n",
       "    'sentence',\n",
       "    'was',\n",
       "    'padded',\n",
       "    'to',\n",
       "    'form',\n",
       "    'a',\n",
       "    'full',\n",
       "    'binary',\n",
       "    'tree',\n",
       "    '.',\n",
       "    'The',\n",
       "    'x',\n",
       "    '-',\n",
       "    'axis',\n",
       "    'represents',\n",
       "    'the',\n",
       "    'number',\n",
       "    'of',\n",
       "    'padding',\n",
       "    'characters',\n",
       "    'introduced',\n",
       "    '.',\n",
       "    'When',\n",
       "    'the',\n",
       "    'padding',\n",
       "    'size',\n",
       "    'is',\n",
       "    'less',\n",
       "    '(',\n",
       "    'up',\n",
       "    'to',\n",
       "    '10',\n",
       "    ')',\n",
       "    ',',\n",
       "    'the',\n",
       "    'NTI',\n",
       "    '-',\n",
       "    'SLSTM',\n",
       "    '-',\n",
       "    'LSTM',\n",
       "    'model',\n",
       "    'performs',\n",
       "    'better',\n",
       "    '.',\n",
       "    'However',\n",
       "    ',',\n",
       "    'this',\n",
       "    'model',\n",
       "    'tends',\n",
       "    'to',\n",
       "    'perform',\n",
       "    'poorly',\n",
       "    'or',\n",
       "    'equally',\n",
       "    'when',\n",
       "    'the',\n",
       "    'padding',\n",
       "    'size',\n",
       "    'is',\n",
       "    'large',\n",
       "    '.',\n",
       "    'Overall',\n",
       "    'we',\n",
       "    'do',\n",
       "    'not',\n",
       "    'observe',\n",
       "    'any',\n",
       "    'significant',\n",
       "    'performance',\n",
       "    'drop',\n",
       "    'for',\n",
       "    'both',\n",
       "    'models',\n",
       "    'as',\n",
       "    'the',\n",
       "    'padding',\n",
       "    'size',\n",
       "    'increases',\n",
       "    '.',\n",
       "    'This',\n",
       "    'suggests',\n",
       "    'that',\n",
       "    'NTI',\n",
       "    'learns',\n",
       "    'to',\n",
       "    'ignore',\n",
       "    'the',\n",
       "    'special',\n",
       "    'padding',\n",
       "    'character',\n",
       "    'while',\n",
       "    'processing',\n",
       "    'padded',\n",
       "    'sentences',\n",
       "    '.',\n",
       "    'The',\n",
       "    'same',\n",
       "    'scenario',\n",
       "    'was',\n",
       "    'also',\n",
       "    'observed',\n",
       "    'while',\n",
       "    'analyzing',\n",
       "    'attention',\n",
       "    'weights',\n",
       "    '.',\n",
       "    'The',\n",
       "    'attention',\n",
       "    'over',\n",
       "    'the',\n",
       "    'padded',\n",
       "    'nodes',\n",
       "    'was',\n",
       "    'nearly',\n",
       "    'zero',\n",
       "    '.'],\n",
       "   'start_pos_in_doc': 4538,\n",
       "   'end_pos_in_doc': 4709,\n",
       "   'ner_dict': {},\n",
       "   'sentence_indices': [(4538, 4544),\n",
       "    (4544, 4558),\n",
       "    (4558, 4570),\n",
       "    (4570, 4592),\n",
       "    (4592, 4603),\n",
       "    (4603, 4615),\n",
       "    (4615, 4637),\n",
       "    (4637, 4654),\n",
       "    (4654, 4672),\n",
       "    (4672, 4688),\n",
       "    (4688, 4699),\n",
       "    (4699, 4709)],\n",
       "   'document_metadata': {'cluster_name_to_id': {},\n",
       "    'span_to_cluster_ids': {},\n",
       "    'relation_to_cluster_ids': {},\n",
       "    'type_to_cluster_ids': {},\n",
       "    'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "    'doc_length': 5013,\n",
       "    'entities_to_features_map': {}},\n",
       "   'num_spans': 0},\n",
       "  {'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "   'paragraph_num': 22,\n",
       "   'paragraph': ['section',\n",
       "    ':',\n",
       "    'Discussion',\n",
       "    'and',\n",
       "    'Conclusion',\n",
       "    'We',\n",
       "    'introduced',\n",
       "    'Neural',\n",
       "    'Tree',\n",
       "    'Indexers',\n",
       "    ',',\n",
       "    'a',\n",
       "    'class',\n",
       "    'of',\n",
       "    'tree',\n",
       "    'structured',\n",
       "    'recursive',\n",
       "    'neural',\n",
       "    'network',\n",
       "    '.',\n",
       "    'The',\n",
       "    'NTI',\n",
       "    'models',\n",
       "    'achieved',\n",
       "    'state',\n",
       "    '-',\n",
       "    'of',\n",
       "    '-',\n",
       "    'the',\n",
       "    '-',\n",
       "    'art',\n",
       "    'performance',\n",
       "    'on',\n",
       "    'different',\n",
       "    'NLP',\n",
       "    'tasks',\n",
       "    '.',\n",
       "    'Most',\n",
       "    'of',\n",
       "    'the',\n",
       "    'NTI',\n",
       "    'models',\n",
       "    'form',\n",
       "    'deep',\n",
       "    'neural',\n",
       "    'networks',\n",
       "    'and',\n",
       "    'we',\n",
       "    'think',\n",
       "    'this',\n",
       "    'is',\n",
       "    'one',\n",
       "    'reason',\n",
       "    'that',\n",
       "    'NTI',\n",
       "    'works',\n",
       "    'well',\n",
       "    'even',\n",
       "    'if',\n",
       "    'it',\n",
       "    'lacks',\n",
       "    'direct',\n",
       "    'linguistic',\n",
       "    'motivations',\n",
       "    'followed',\n",
       "    'by',\n",
       "    'other',\n",
       "    'syntactic',\n",
       "    '-',\n",
       "    'tree',\n",
       "    '-',\n",
       "    'structured',\n",
       "    'recursive',\n",
       "    'models',\n",
       "    '.',\n",
       "    'CNN',\n",
       "    'and',\n",
       "    'NTI',\n",
       "    'are',\n",
       "    'topologically',\n",
       "    'related',\n",
       "    '.',\n",
       "    'Both',\n",
       "    'NTI',\n",
       "    'and',\n",
       "    'CNNs',\n",
       "    'are',\n",
       "    'hierarchical',\n",
       "    '.',\n",
       "    'However',\n",
       "    ',',\n",
       "    'current',\n",
       "    'implementation',\n",
       "    'of',\n",
       "    'NTI',\n",
       "    'only',\n",
       "    'operates',\n",
       "    'on',\n",
       "    'non',\n",
       "    '-',\n",
       "    'overlapping',\n",
       "    'sub',\n",
       "    '-',\n",
       "    'trees',\n",
       "    'while',\n",
       "    'CNNs',\n",
       "    'can',\n",
       "    'slide',\n",
       "    'over',\n",
       "    'the',\n",
       "    'input',\n",
       "    'to',\n",
       "    'produce',\n",
       "    'higher',\n",
       "    '-',\n",
       "    'level',\n",
       "    'representations',\n",
       "    '.',\n",
       "    'NTI',\n",
       "    'is',\n",
       "    'flexible',\n",
       "    'in',\n",
       "    'selecting',\n",
       "    'the',\n",
       "    'node',\n",
       "    'function',\n",
       "    'and',\n",
       "    'the',\n",
       "    'attention',\n",
       "    'mechanism',\n",
       "    '.',\n",
       "    'Like',\n",
       "    'CNN',\n",
       "    ',',\n",
       "    'the',\n",
       "    'computation',\n",
       "    'in',\n",
       "    'the',\n",
       "    'same',\n",
       "    'tree',\n",
       "    '-',\n",
       "    'depth',\n",
       "    'can',\n",
       "    'be',\n",
       "    'parallelized',\n",
       "    'effectively',\n",
       "    ';',\n",
       "    'and',\n",
       "    'therefore',\n",
       "    'NTI',\n",
       "    'is',\n",
       "    'scalable',\n",
       "    'and',\n",
       "    'suitable',\n",
       "    'for',\n",
       "    'large',\n",
       "    '-',\n",
       "    'scale',\n",
       "    'sequence',\n",
       "    'processing',\n",
       "    '.',\n",
       "    'Note',\n",
       "    'that',\n",
       "    'NTI',\n",
       "    'can',\n",
       "    'be',\n",
       "    'seen',\n",
       "    'as',\n",
       "    'a',\n",
       "    'generalization',\n",
       "    'of',\n",
       "    'LSTM',\n",
       "    '.',\n",
       "    'If',\n",
       "    'we',\n",
       "    'construct',\n",
       "    'left',\n",
       "    '-',\n",
       "    'branching',\n",
       "    'trees',\n",
       "    'in',\n",
       "    'a',\n",
       "    'bottom',\n",
       "    '-',\n",
       "    'up',\n",
       "    'fashion',\n",
       "    ',',\n",
       "    'the',\n",
       "    'model',\n",
       "    'acts',\n",
       "    'just',\n",
       "    'like',\n",
       "    'sequential',\n",
       "    'LSTM',\n",
       "    '.',\n",
       "    'Different',\n",
       "    'branching',\n",
       "    'factors',\n",
       "    'for',\n",
       "    'the',\n",
       "    'underlying',\n",
       "    'tree',\n",
       "    'structure',\n",
       "    'have',\n",
       "    'yet',\n",
       "    'to',\n",
       "    'be',\n",
       "    'explored',\n",
       "    '.',\n",
       "    'NTI',\n",
       "    'can',\n",
       "    'be',\n",
       "    'extended',\n",
       "    'so',\n",
       "    'it',\n",
       "    'learns',\n",
       "    'to',\n",
       "    'select',\n",
       "    'and',\n",
       "    'compose',\n",
       "    'dynamic',\n",
       "    'number',\n",
       "    'of',\n",
       "    'nodes',\n",
       "    'for',\n",
       "    'efficiency',\n",
       "    ',',\n",
       "    'essentially',\n",
       "    'discovering',\n",
       "    'intrinsic',\n",
       "    'hierarchical',\n",
       "    'structure',\n",
       "    'in',\n",
       "    'the',\n",
       "    'input',\n",
       "    '.'],\n",
       "   'start_pos_in_doc': 4709,\n",
       "   'end_pos_in_doc': 4945,\n",
       "   'ner_dict': {},\n",
       "   'sentence_indices': [(4709, 4714),\n",
       "    (4714, 4729),\n",
       "    (4729, 4746),\n",
       "    (4746, 4784),\n",
       "    (4784, 4791),\n",
       "    (4791, 4798),\n",
       "    (4798, 4827),\n",
       "    (4827, 4840),\n",
       "    (4840, 4870),\n",
       "    (4870, 4882),\n",
       "    (4882, 4904),\n",
       "    (4904, 4918),\n",
       "    (4918, 4945)],\n",
       "   'document_metadata': {'cluster_name_to_id': {},\n",
       "    'span_to_cluster_ids': {},\n",
       "    'relation_to_cluster_ids': {},\n",
       "    'type_to_cluster_ids': {},\n",
       "    'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "    'doc_length': 5013,\n",
       "    'entities_to_features_map': {}},\n",
       "   'num_spans': 0},\n",
       "  {'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "   'paragraph_num': 23,\n",
       "   'paragraph': ['section',\n",
       "    ':',\n",
       "    'Acknowledgments',\n",
       "    'We',\n",
       "    'would',\n",
       "    'like',\n",
       "    'to',\n",
       "    'thank',\n",
       "    'the',\n",
       "    'anonymous',\n",
       "    'reviewers',\n",
       "    'for',\n",
       "    'their',\n",
       "    'insightful',\n",
       "    'comments',\n",
       "    'and',\n",
       "    'suggestions',\n",
       "    '.',\n",
       "    'This',\n",
       "    'work',\n",
       "    'was',\n",
       "    'supported',\n",
       "    'in',\n",
       "    'part',\n",
       "    'by',\n",
       "    'the',\n",
       "    'grant',\n",
       "    'HL125089',\n",
       "    'from',\n",
       "    'the',\n",
       "    'National',\n",
       "    'Institutes',\n",
       "    'of',\n",
       "    'Health',\n",
       "    '(',\n",
       "    'NIH',\n",
       "    ')',\n",
       "    '.',\n",
       "    'Any',\n",
       "    'opinions',\n",
       "    ',',\n",
       "    'findings',\n",
       "    'and',\n",
       "    'conclusions',\n",
       "    'or',\n",
       "    'recommendations',\n",
       "    'expressed',\n",
       "    'in',\n",
       "    'this',\n",
       "    'material',\n",
       "    'are',\n",
       "    'those',\n",
       "    'of',\n",
       "    'the',\n",
       "    'authors',\n",
       "    'and',\n",
       "    'do',\n",
       "    'not',\n",
       "    'necessarily',\n",
       "    'reflect',\n",
       "    'those',\n",
       "    'of',\n",
       "    'the',\n",
       "    'sponsor',\n",
       "    '.'],\n",
       "   'start_pos_in_doc': 4945,\n",
       "   'end_pos_in_doc': 5010,\n",
       "   'ner_dict': {},\n",
       "   'sentence_indices': [(4945, 4948),\n",
       "    (4948, 4963),\n",
       "    (4963, 4983),\n",
       "    (4983, 5010)],\n",
       "   'document_metadata': {'cluster_name_to_id': {},\n",
       "    'span_to_cluster_ids': {},\n",
       "    'relation_to_cluster_ids': {},\n",
       "    'type_to_cluster_ids': {},\n",
       "    'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "    'doc_length': 5013,\n",
       "    'entities_to_features_map': {}},\n",
       "   'num_spans': 0},\n",
       "  {'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "   'paragraph_num': 24,\n",
       "   'paragraph': ['bibliography', ':', 'References'],\n",
       "   'start_pos_in_doc': 5010,\n",
       "   'end_pos_in_doc': 5013,\n",
       "   'ner_dict': {},\n",
       "   'sentence_indices': [(5010, 5013)],\n",
       "   'document_metadata': {'cluster_name_to_id': {},\n",
       "    'span_to_cluster_ids': {},\n",
       "    'relation_to_cluster_ids': {},\n",
       "    'type_to_cluster_ids': {},\n",
       "    'doc_id': '270e65acc071b9e4e2a632720130c0e10cb6fa08',\n",
       "    'doc_length': 5013,\n",
       "    'entities_to_features_map': {}},\n",
       "   'num_spans': 0}],\n",
       " 'spans': tensor([[[-1, -1]],\n",
       " \n",
       "         [[-1, -1]],\n",
       " \n",
       "         [[-1, -1]],\n",
       " \n",
       "         [[-1, -1]],\n",
       " \n",
       "         [[-1, -1]]], device='cuda:0'),\n",
       " 'span_cluster_labels': tensor([], device='cuda:0', size=(5, 1, 0), dtype=torch.int64),\n",
       " 'span_saliency_labels': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]], device='cuda:0'),\n",
       " 'span_type_labels': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]], device='cuda:0'),\n",
       " 'span_features': tensor([[[0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0]]], device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def span_embeddings_forward(self, output_embedding, spans, span_type_labels, span_features, metadata):\n",
    "        \n",
    "\n",
    "        if spans.nelement() != 0:\n",
    "            span_mask, spans, span_embeddings = self.extract_span_embeddings(\n",
    "                output_embedding[\"contextualised\"], spans\n",
    "            )\n",
    "\n",
    "            if span_mask.sum() != 0:\n",
    "                span_offset = self.offset_span_by_para_start(metadata, spans, span_mask)\n",
    "                span_position = self.get_span_position(metadata, span_offset)\n",
    "                span_type_labels_one_hot = self.get_span_one_hot_labels(\n",
    "                    \"span_type_labels\", span_type_labels, spans\n",
    "                )\n",
    "\n",
    "                span_features = torch.cat(\n",
    "                    [span_position, span_type_labels_one_hot, span_features.float()], dim=-1\n",
    "                )\n",
    "                featured_span_embeddings = torch.cat([span_embeddings, span_features], dim=-1)\n",
    "                span_ix = span_mask.view(-1).nonzero().squeeze(1).long()\n",
    "\n",
    "                output_span_embeddings = {\n",
    "                    \"span_mask\": span_mask,\n",
    "                    \"span_ix\": span_ix,\n",
    "                    \"spans\": span_offset,\n",
    "                    \"span_embeddings\": span_embeddings,\n",
    "                    \"featured_span_embeddings\": featured_span_embeddings,\n",
    "                    \"span_type_labels\": span_type_labels_one_hot,\n",
    "                    \"span_features\": span_features.float(),\n",
    "                    \"valid\": True,\n",
    "                }\n",
    "\n",
    "        return output_span_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ec5449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2631010101010101"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saliency_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "842e3ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232b43584b2236669c0a53702ad89ab10c3886ea\n",
      "[]\n",
      "23d2d3a6ffebfecaa8930307fdcf451c147757c8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "23d2d3a6ffebfecaa8930307fdcf451c147757c8\n",
      "[]\n",
      "23d2d3a6ffebfecaa8930307fdcf451c147757c8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "24730424236724d3f798dec02901e7a1f1c4710e\n",
      "[]\n",
      "24730424236724d3f798dec02901e7a1f1c4710e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "24730424236724d3f798dec02901e7a1f1c4710e\n",
      "[]\n",
      "24730424236724d3f798dec02901e7a1f1c4710e\n",
      "[]\n",
      "249b3b7421d3cdb932eecfe4b67203e0e46806b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:01, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "249b3b7421d3cdb932eecfe4b67203e0e46806b2\n",
      "[]\n",
      "249b3b7421d3cdb932eecfe4b67203e0e46806b2\n",
      "[]\n",
      "25a784f7f8c94c42821ee078587fc38dffcd00a4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [00:01, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "25a784f7f8c94c42821ee078587fc38dffcd00a4\n",
      "[]\n",
      "25a784f7f8c94c42821ee078587fc38dffcd00a4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [00:01,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "25f5df29342a04936ba0d308b4d1b8245a7e8f5c\n",
      "[]\n",
      "25f5df29342a04936ba0d308b4d1b8245a7e8f5c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:01, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "25f5df29342a04936ba0d308b4d1b8245a7e8f5c\n",
      "[]\n",
      "269730dbbabed8b8b5ba720e44a4c31b1f51e8f1\n",
      "[]\n",
      "269730dbbabed8b8b5ba720e44a4c31b1f51e8f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [00:02,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "269730dbbabed8b8b5ba720e44a4c31b1f51e8f1\n",
      "[]\n",
      "269730dbbabed8b8b5ba720e44a4c31b1f51e8f1\n",
      "[]\n",
      "270e65acc071b9e4e2a632720130c0e10cb6fa08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:02,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "270e65acc071b9e4e2a632720130c0e10cb6fa08\n",
      "[]\n",
      "270e65acc071b9e4e2a632720130c0e10cb6fa08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/209030495.tmpdir/ipykernel_50681/1992588487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Put on GPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moutput_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaliency_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoded_spans'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#metadata = output_res['metadata']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/scratch/fgonzalez/SciREX/scirex/models/scirex_model.py\u001b[0m in \u001b[0;36mdecode_saliency\u001b[0;34m(self, batch, saliency_threshold)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaliency_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0moutput_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         output_span_embedding = self.span_embeddings_forward(\n\u001b[1;32m    350\u001b[0m             \u001b[0moutput_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/scratch/fgonzalez/SciREX/scirex/models/scirex_model.py\u001b[0m in \u001b[0;36membedding_forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    130\u001b[0m         filtered_contextualized_embeddings = self._context_layer(\n\u001b[1;32m    131\u001b[0m             \u001b[0mfiltered_text_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_text_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiltered_text_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         ).squeeze(0)\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/allennlp/modules/seq2seq_encoders/pytorch_seq2seq_wrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mpacked_sequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_and_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0munpacked_sequence_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/allennlp/modules/encoder_base.py\u001b[0m in \u001b[0;36msort_and_run_forward\u001b[0;34m(self, module, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Actually call the module on the sorted PackedSequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 580\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in tqdm(iterator):\n",
    "    #print(batch.keys())\n",
    "    batch = nn_util.move_to_device(batch, cuda_device)  # Put on GPU.\n",
    "    print(batch['metadata'][0]['doc_id'])\n",
    "    output_res = model.decode_saliency(batch, saliency_threshold)\n",
    "    print(output_res['decoded_spans'])\n",
    "    #metadata = output_res['metadata']\n",
    "    #print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(iterator):\n",
    "    print(b)\n",
    "    batch = nn_util.move_to_device(batch, cuda_device)  # Put on GPU.\n",
    "    output_res = model.decode_saliency(batch, saliency_threshold)\n",
    "\n",
    "    metadata = output_res['metadata']\n",
    "    doc_ids: List[str] = [m[\"doc_id\"] for m in metadata]\n",
    "    assert len(set(doc_ids)) == 1\n",
    "\n",
    "    decoded_spans: List[Dict[tuple, float]] = output_res['decoded_spans']\n",
    "    doc_id = metadata[0]['doc_id']\n",
    "\n",
    "    if doc_id not in documents :\n",
    "        documents[doc_id] = {}\n",
    "        documents[doc_id]['saliency'] = []\n",
    "        documents[doc_id]['doc_id'] = doc_id\n",
    "\n",
    "    for pspans in decoded_spans :\n",
    "        for span, prob in pspans.items() :\n",
    "            documents[doc_id]['saliency'].append([span[0], span[1], 1 if prob > saliency_threshold else 0, prob])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8890600",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(output_file, \"w\") as f:\n",
    "        documents = {}\n",
    "        for batch in tqdm(iterator):\n",
    "            batch = nn_util.move_to_device(batch, cuda_device)  # Put on GPU.\n",
    "            output_res = model.decode_saliency(batch, saliency_threshold)\n",
    "            \n",
    "            metadata = output_res['metadata']\n",
    "            doc_ids: List[str] = [m[\"doc_id\"] for m in metadata]\n",
    "            assert len(set(doc_ids)) == 1\n",
    "\n",
    "            decoded_spans: List[Dict[tuple, float]] = output_res['decoded_spans']\n",
    "            doc_id = metadata[0]['doc_id']\n",
    "\n",
    "            if doc_id not in documents :\n",
    "                documents[doc_id] = {}\n",
    "                documents[doc_id]['saliency'] = []\n",
    "                documents[doc_id]['doc_id'] = doc_id\n",
    "\n",
    "            for pspans in decoded_spans :\n",
    "                for span, prob in pspans.items() :\n",
    "                    documents[doc_id]['saliency'].append([span[0], span[1], 1 if prob > saliency_threshold else 0, prob])\n",
    "\n",
    "        f.write(\"\\n\".join([json.dumps(x) for x in documents.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "454d8b54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_id': '455da02e5048dffb51fb6ab5eb8aeca5926c9d9a', 'paragraph_num': 47, 'paragraph': ['bibliography', ':', 'References', 'section', ':', 'Changelog', 'arXiv', 'v1', '.', 'Initial', 'technical', 'report', 'for', 'ECCV', '2014', 'paper', '.', 'arXiv', 'v2', '.', 'Submitted', 'version', 'for', 'TPAMI', '.', 'Includes', 'extra', 'experiments', 'of', 'SPP', 'on', 'various', 'architectures', '.', 'Includes', 'details', 'for', 'ILSVRC', '2014', '.', 'arXiv', 'v3', '.', 'Accepted', 'version', 'for', 'TPAMI', '.', 'Includes', 'comparisons', 'with', 'R', '-', 'CNN', 'using', 'the', 'same', 'architecture', '.', 'Includes', 'detection', 'experiments', 'using', 'EdgeBoxes', '.', 'arXiv', 'v4', '.', 'Revised', '“', 'Mapping', 'a', 'Window', 'to', 'Feature', 'Maps', '”', 'in', 'Appendix', 'for', 'easier', 'implementation', '.'], 'start_pos_in_doc': 9779, 'end_pos_in_doc': 9862, 'ner_dict': {}, 'sentence_indices': [(9779, 9782), (9782, 9785), (9785, 9788), (9788, 9796), (9796, 9797), (9797, 9799), (9799, 9804), (9804, 9813), (9813, 9820), (9820, 9822), (9822, 9827), (9827, 9838), (9838, 9844), (9844, 9847), (9847, 9849), (9849, 9862)], 'document_metadata': {'cluster_name_to_id': {}, 'span_to_cluster_ids': {}, 'relation_to_cluster_ids': {}, 'type_to_cluster_ids': {}, 'doc_id': '455da02e5048dffb51fb6ab5eb8aeca5926c9d9a', 'doc_length': 9862, 'entities_to_features_map': {}}, 'num_spans': 0}\n"
     ]
    }
   ],
   "source": [
    "print(instance['metadata'].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79624c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(archive_folder, test_file, output_file, cuda_device):\n",
    "    '''\n",
    "    test_file contains atleast - doc_id, sections, sentences, ner in scirex format.\n",
    "\n",
    "    output_file - {\n",
    "        'doc_id' : str,\n",
    "        'saliency' : Tuple[start_index, end_index, salient (binary), saliency probability]\n",
    "    }\n",
    "    '''\n",
    "    import_submodules(\"scirex\")\n",
    "    logging.info(\"Loading Model from %s\", archive_folder)\n",
    "    archive_file = os.path.join(archive_folder, \"model.tar.gz\")\n",
    "    archive = load_archive(archive_file, cuda_device)\n",
    "    model = archive.model\n",
    "    model.eval()\n",
    "\n",
    "    saliency_threshold = json.load(open(archive_folder + '/metrics.json'))['best_validation__span_threshold']\n",
    "\n",
    "    model.prediction_mode = True\n",
    "    config = archive.config.duplicate()\n",
    "    dataset_reader_params = config[\"dataset_reader\"]\n",
    "    dataset_reader = DatasetReader.from_params(dataset_reader_params)\n",
    "    dataset_reader.prediction_mode = True\n",
    "    instances = dataset_reader.read(test_file)\n",
    "\n",
    "    for instance in instances :\n",
    "        batch = Batch([instance])\n",
    "        batch.index_instances(model.vocab)\n",
    "\n",
    "    data_iterator = DataIterator.from_params(config[\"validation_iterator\"])\n",
    "    iterator = data_iterator(instances, num_epochs=1, shuffle=False)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        documents = {}\n",
    "        for batch in tqdm(iterator):\n",
    "            batch = nn_util.move_to_device(batch, cuda_device)  # Put on GPU.\n",
    "            output_res = model.decode_saliency(batch, saliency_threshold)\n",
    "            \n",
    "            metadata = output_res['metadata']\n",
    "            doc_ids: List[str] = [m[\"doc_id\"] for m in metadata]\n",
    "            assert len(set(doc_ids)) == 1\n",
    "\n",
    "            decoded_spans: List[Dict[tuple, float]] = output_res['decoded_spans']\n",
    "            doc_id = metadata[0]['doc_id']\n",
    "\n",
    "            if doc_id not in documents :\n",
    "                documents[doc_id] = {}\n",
    "                documents[doc_id]['saliency'] = []\n",
    "                documents[doc_id]['doc_id'] = doc_id\n",
    "\n",
    "            for pspans in decoded_spans :\n",
    "                for span, prob in pspans.items() :\n",
    "                    documents[doc_id]['saliency'].append([span[0], span[1], 1 if prob > saliency_threshold else 0, prob])\n",
    "\n",
    "        f.write(\"\\n\".join([json.dumps(x) for x in documents.values()]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba45cb",
   "metadata": {},
   "source": [
    "### Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68fc045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import (\n",
    "    ListField,\n",
    "    MetadataField,\n",
    "    SequenceLabelField,\n",
    "    SpanField,\n",
    "    TextField,\n",
    ")\n",
    "from scirex.data.dataset_readers.multi_label_field import MultiLabelField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from overrides import overrides\n",
    "\n",
    "from scirex.data.utils.paragraph_alignment import *\n",
    "from scirex.data.utils.section_feature_extraction import extract_sentence_features\n",
    "from scirex.data.utils.span_utils import does_overlap, is_x_in_y, spans_to_bio_tags\n",
    "from scirex_utilities.entity_utils import used_entities\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "Span = Tuple[int, int]  # eg. (0, 10)\n",
    "ClusterName = str\n",
    "BaseEntityType = str  # eg. Method\n",
    "EntityType = Tuple[str, str]  # eg. (Method, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = dataset_reader.read(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1316e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b69538b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_paragraph_length = 300\n",
    "lazy = False\n",
    "to_scirex_converter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45115468",
   "metadata": {},
   "outputs": [],
   "source": [
    "    type: 'scirex_full_reader',\n",
    "    token_indexers: token_indexers,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb53d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'token_indexers': {'bert': {'do_lowercase': 'true', 'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt', 'truncate_long_sequences': False, 'type': 'bert-pretrained', 'use_starting_offsets': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "10411478",
   "metadata": {},
   "outputs": [],
   "source": [
    "_token_indexers: Dict[str, TokenIndexer] = token_indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b0208541",
   "metadata": {},
   "outputs": [],
   "source": [
    "_token_indexers={\"bert\": TokenIndexer(token_indexers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "feb39967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.token_indexers.token_indexer.TokenIndexer at 0x2adc4fe8be90>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_token_indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "70449c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexers={\"token_indexers\" : {\n",
    "    \"bert\": {\n",
    "      \"type\": \"bert-pretrained\",\n",
    "      \"pretrained_model\": '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt',\n",
    "      \"do_lowercase\": 'true',\n",
    "      \"use_starting_offsets\": True,\n",
    "      \"truncate_long_sequences\" : False\n",
    "    }\n",
    "  }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "115e3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexers = {\n",
    "    \"bert\": {\n",
    "      \"type\": \"bert-pretrained\",\n",
    "      \"pretrained_model\": '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt',\n",
    "      \"do_lowercase\": 'true',\n",
    "      \"use_starting_offsets\": True,\n",
    "      \"truncate_long_sequences\" : False\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09edd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "_max_paragraph_length = max_paragraph_length\n",
    "prediction_mode = False\n",
    "\n",
    "## Hack so I can reuse same reader to convert scirex\n",
    "## format to scierc format\n",
    "to_scierc_converter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a9f546e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_token_indexers = token_indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c039e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as g:\n",
    "    for _, line in enumerate(g):\n",
    "        json_dict = json.loads(line)\n",
    "        if prediction_mode:\n",
    "            if \"method_subrelations\" in json_dict:\n",
    "                del json_dict[\"method_subrelations\"]\n",
    "            json_dict[\"n_ary_relations\"] = []\n",
    "        json_dict = clean_json_dict(json_dict)\n",
    "\n",
    "        verify_json_dict(json_dict)\n",
    "\n",
    "        # Get fields from JSON dict\n",
    "        doc_id = json_dict[\"doc_id\"]\n",
    "        sections: List[Span] = json_dict[\"sections\"]\n",
    "        sentences: List[List[Span]] = json_dict[\"sentences\"]\n",
    "        words: List[str] = json_dict[\"words\"]\n",
    "        entities: Dict[Span, EntityType] = json_dict[\"ner\"]\n",
    "        corefs: Dict[ClusterName, List[Span]] = json_dict[\"coref\"]\n",
    "        n_ary_relations: List[Dict[BaseEntityType, ClusterName]] = {}#json_dict[\"n_ary_relations\"]\n",
    "\n",
    "        # Extract Document structure features\n",
    "        entities_to_features_map: Dict[Span, List[str]] = extract_sentence_features(\n",
    "            sentences, words, entities\n",
    "        )\n",
    "\n",
    "        # Map cluster names to integer cluster ids\n",
    "        cluster_name_to_id: Dict[ClusterName, int] = {\n",
    "            k: i for i, k in enumerate(sorted(list(corefs.keys())))\n",
    "        }\n",
    "        max_salient_cluster = len(corefs)\n",
    "\n",
    "        # Map Spans to list of clusters ids it belong to.\n",
    "        span_to_cluster_ids: Dict[Span, List[int]] = {}\n",
    "        for cluster_name in corefs:\n",
    "            for span in corefs[cluster_name]:\n",
    "                span_to_cluster_ids.setdefault(span, []).append(cluster_name_to_id[cluster_name])\n",
    "\n",
    "        span_to_cluster_ids = {span: sorted(v) for span, v in span_to_cluster_ids.items()}\n",
    "\n",
    "        assert sorted(list(cluster_name_to_id.values())) == list(\n",
    "            range(max_salient_cluster)\n",
    "        ), breakpoint()\n",
    "\n",
    "        # Map types to list of cluster ids that are of that type\n",
    "        type_to_cluster_ids: Dict[BaseEntityType, List[int]] = {k: [] for k in used_entities}\n",
    "\n",
    "        for cluster_name in corefs:\n",
    "            types = [entities[span][0] for span in corefs[cluster_name]]\n",
    "            if len(set(types)) > 0:\n",
    "                try :\n",
    "                    type_to_cluster_ids[mode(types)[0][0]].append(cluster_name_to_id[cluster_name])\n",
    "                except :\n",
    "                    # SciERC gives trouble here. Not relevant .\n",
    "                    continue\n",
    "\n",
    "        # Map relations to list of cluster ids in it.\n",
    "        relation_to_cluster_ids: Dict[int, List[int]] = {}\n",
    "        for rel_idx, rel in enumerate(n_ary_relations):\n",
    "            relation_to_cluster_ids[rel_idx] = []\n",
    "            for entity in used_entities:\n",
    "                relation_to_cluster_ids[rel_idx].append(cluster_name_to_id[rel[entity]])\n",
    "                type_to_cluster_ids[entity].append(cluster_name_to_id[rel[entity]])\n",
    "\n",
    "            relation_to_cluster_ids[rel_idx] = tuple(relation_to_cluster_ids[rel_idx])\n",
    "\n",
    "        for k in type_to_cluster_ids:\n",
    "            type_to_cluster_ids[k] = sorted(list(set(type_to_cluster_ids[k])))\n",
    "\n",
    "        # Move paragraph boundaries around to accomodate in BERT\n",
    "        sections, sentences_grouped, entities_grouped = resize_sections_and_group(\n",
    "            sections, sentences, entities\n",
    "        )\n",
    "\n",
    "        document_metadata = {\n",
    "            \"cluster_name_to_id\": cluster_name_to_id,\n",
    "            \"span_to_cluster_ids\": span_to_cluster_ids,\n",
    "            \"relation_to_cluster_ids\": relation_to_cluster_ids,\n",
    "            \"type_to_cluster_ids\": type_to_cluster_ids,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"doc_length\": len(words),\n",
    "            \"entities_to_features_map\": entities_to_features_map,\n",
    "        }\n",
    "\n",
    "        # Loop over the sections.\n",
    "        for (paragraph_num, ((start_ix, end_ix), sentences, ner_dict)) in enumerate(\n",
    "            zip(sections, sentences_grouped, entities_grouped)\n",
    "        ):\n",
    "            paragraph = words[start_ix:end_ix]\n",
    "            if len(paragraph) == 0:\n",
    "                breakpoint()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ae4743b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['words', 'sections', 'sentences', 'predicted_ner', 'doc_id', 'ner', 'coref'])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aa7ea92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M.S.',\n",
       " 'degrees',\n",
       " 'in',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'from',\n",
       " 'Nanjing',\n",
       " 'University',\n",
       " ',',\n",
       " 'China',\n",
       " ',',\n",
       " 'in',\n",
       " '2001',\n",
       " 'and',\n",
       " '2004',\n",
       " ',',\n",
       " 'respectively',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Ph',\n",
       " '.',\n",
       " 'D',\n",
       " 'degree',\n",
       " 'from',\n",
       " 'Deakin',\n",
       " 'University',\n",
       " ',',\n",
       " 'Australia',\n",
       " 'in',\n",
       " '2008',\n",
       " '.',\n",
       " 'He',\n",
       " 'joined',\n",
       " 'the',\n",
       " 'School',\n",
       " 'of',\n",
       " 'Computer',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Engineering',\n",
       " 'at',\n",
       " 'Southeast',\n",
       " 'University',\n",
       " ',',\n",
       " 'China',\n",
       " ',',\n",
       " 'in',\n",
       " '2008',\n",
       " ',',\n",
       " 'and',\n",
       " 'is',\n",
       " 'currently',\n",
       " 'a',\n",
       " 'professor',\n",
       " 'and',\n",
       " 'vice',\n",
       " 'dean',\n",
       " 'of',\n",
       " 'the',\n",
       " 'school',\n",
       " '.',\n",
       " 'He',\n",
       " 'has',\n",
       " 'authored',\n",
       " 'over',\n",
       " '50',\n",
       " 'refereed',\n",
       " 'papers',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'holds',\n",
       " 'five',\n",
       " 'patents',\n",
       " 'in',\n",
       " 'these',\n",
       " 'areas',\n",
       " '.',\n",
       " 'His',\n",
       " 'research',\n",
       " 'interests',\n",
       " 'include',\n",
       " 'pattern',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'machine',\n",
       " 'learning',\n",
       " ',',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'vision',\n",
       " '.']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e9e0190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n"
     ]
    }
   ],
   "source": [
    "for indexer_name, indexer in _token_indexers.items():\n",
    "    print(indexer_name)\n",
    "    array_keys = indexer.get_keys(indexer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3f90658c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/scratch/209030495.tmpdir/ipykernel_50681/287814163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'get_keys'"
     ]
    }
   ],
   "source": [
    "indexer.get_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c81bc1c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/scratch/209030495.tmpdir/ipykernel_50681/4219324371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_token_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'get_keys'"
     ]
    }
   ],
   "source": [
    "_token_indexers.get_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "77f6b4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('bert', {'type': 'bert-pretrained', 'pretrained_model': '/cluster/scratch/fgonzalez/scibert/scibert_scivocab_uncased//vocab.txt', 'do_lowercase': 'true', 'use_starting_offsets': True, 'truncate_long_sequences': False})])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_token_indexers.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1c260b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = text_to_instance(\n",
    "                        paragraph_num=paragraph_num,\n",
    "                        paragraph=paragraph,\n",
    "                        ner_dict=ner_dict,\n",
    "                        start_ix=start_ix,\n",
    "                        end_ix=end_ix,\n",
    "                        sentence_indices=sentences,\n",
    "                        document_metadata=document_metadata,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "167d84cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <allennlp.data.fields.text_field.TextField at 0x2adc505784d0>,\n",
       " 'ner_type_labels': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x2adc50578890>,\n",
       " 'metadata': <allennlp.data.fields.metadata_field.MetadataField at 0x2adc505787d0>,\n",
       " 'spans': <allennlp.data.fields.list_field.ListField at 0x2adc50578210>,\n",
       " 'span_cluster_labels': <allennlp.data.fields.list_field.ListField at 0x2adc50578b90>,\n",
       " 'span_saliency_labels': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x2adc505781d0>,\n",
       " 'span_type_labels': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x2adc50578410>,\n",
       " 'span_features': <allennlp.data.fields.list_field.ListField at 0x2adc4fadd650>}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "be646b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_scierc_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2356204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_num=paragraph_num\n",
    "paragraph=paragraph\n",
    "ner_dict=ner_dict\n",
    "start_ix=start_ix\n",
    "end_ix=end_ix\n",
    "sentence_indices=sentences\n",
    "document_metadata=document_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f38aced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = TextField([Token(word) for word in paragraph], _token_indexers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2ba9df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field = MetadataField(\n",
    "    dict(\n",
    "        doc_id=document_metadata[\"doc_id\"],\n",
    "        paragraph_num=paragraph_num,\n",
    "        paragraph=paragraph,\n",
    "        start_pos_in_doc=start_ix,\n",
    "        end_pos_in_doc=end_ix,\n",
    "        ner_dict=ner_dict,\n",
    "        sentence_indices=sentence_indices,\n",
    "        document_metadata=document_metadata,\n",
    "        num_spans=len(ner_dict),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b18bc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_type_labels = spans_to_bio_tags(\n",
    "    [(k[0] - start_ix, k[1] - start_ix, v[0]) for k, v in ner_dict.items()], len(paragraph)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "af3390f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5699faaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(ner_type_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ff20f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_instance(\n",
    "        paragraph_num: int,\n",
    "        paragraph: List[str],\n",
    "        ner_dict: Dict[Span, str],\n",
    "        start_ix: int,\n",
    "        end_ix: int,\n",
    "        sentence_indices: List[Span],\n",
    "        document_metadata: Dict[str, Any],\n",
    "    ):\n",
    "\n",
    "        if to_scierc_converter:\n",
    "            return dict(\n",
    "                paragraph_num=paragraph_num,\n",
    "                paragraph=paragraph,\n",
    "                ner_dict=ner_dict,\n",
    "                start_ix=start_ix,\n",
    "                end_ix=end_ix,\n",
    "                sentence_indices=sentence_indices,\n",
    "                document_metadata=document_metadata,\n",
    "            )\n",
    "\n",
    "        text_field = TextField([Token(word) for word in paragraph], _token_indexers)\n",
    "\n",
    "        metadata_field = MetadataField(\n",
    "            dict(\n",
    "                doc_id=document_metadata[\"doc_id\"],\n",
    "                paragraph_num=paragraph_num,\n",
    "                paragraph=paragraph,\n",
    "                start_pos_in_doc=start_ix,\n",
    "                end_pos_in_doc=end_ix,\n",
    "                ner_dict=ner_dict,\n",
    "                sentence_indices=sentence_indices,\n",
    "                document_metadata=document_metadata,\n",
    "                num_spans=len(ner_dict),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ner_type_labels = spans_to_bio_tags(\n",
    "            [(k[0] - start_ix, k[1] - start_ix, v[0]) for k, v in ner_dict.items()], len(paragraph)\n",
    "        )\n",
    "\n",
    "        ner_entity_field = SequenceLabelField(ner_type_labels, text_field, label_namespace=\"ner_type_labels\")\n",
    "\n",
    "        # Pull it  all together.\n",
    "        fields = dict(text=text_field, ner_type_labels=ner_entity_field, metadata=metadata_field)\n",
    "\n",
    "        spans = []\n",
    "        span_cluster_labels = []\n",
    "        span_saliency_labels = []\n",
    "        span_type_labels = []\n",
    "        span_features = []\n",
    "\n",
    "        entities_to_features_map = document_metadata[\"entities_to_features_map\"]\n",
    "        cluster_name_to_id = document_metadata[\"cluster_name_to_id\"]\n",
    "        relation_to_cluster_ids = document_metadata[\"relation_to_cluster_ids\"]\n",
    "        span_to_cluster_ids = document_metadata[\"span_to_cluster_ids\"]\n",
    "\n",
    "        for (s, e), label in ner_dict.items():\n",
    "            spans.append(SpanField(int(s - start_ix), int(e - start_ix - 1), text_field))\n",
    "            span_cluster_labels.append(\n",
    "                MultiLabelField(\n",
    "                    span_to_cluster_ids.get((s, e), []),\n",
    "                    label_namespace=\"cluster_labels\",\n",
    "                    skip_indexing=True,\n",
    "                    num_labels=len(cluster_name_to_id),\n",
    "                )\n",
    "            )\n",
    "            span_saliency_labels.append(1 if label[-1] == \"True\" else 0)\n",
    "            span_type_labels.append(label[0])\n",
    "            span_features.append(\n",
    "                MultiLabelField(entities_to_features_map[(s, e)], label_namespace=\"section_feature_labels\", num_labels=5)\n",
    "            )\n",
    "\n",
    "        if len(spans) > 0:\n",
    "            fields[\"spans\"] = ListField(spans)\n",
    "            fields[\"span_cluster_labels\"] = ListField(span_cluster_labels)\n",
    "            fields[\"span_saliency_labels\"] = SequenceLabelField(\n",
    "                span_saliency_labels, fields[\"spans\"], label_namespace=\"span_saliency_labels\"\n",
    "            )\n",
    "            fields[\"span_type_labels\"] = SequenceLabelField(\n",
    "                span_type_labels, fields[\"spans\"], label_namespace=\"span_type_labels\"\n",
    "            )\n",
    "            fields[\"span_features\"] = ListField(span_features)\n",
    "        else:  # Some paragraphs may not have anything !\n",
    "            fields[\"spans\"] = ListField([SpanField(-1, -1, text_field).empty_field()]).empty_field()\n",
    "            fields[\"span_cluster_labels\"] = ListField(\n",
    "                [\n",
    "                    MultiLabelField(\n",
    "                        [],\n",
    "                        label_namespace=\"cluster_labels\",\n",
    "                        skip_indexing=True,\n",
    "                        num_labels=len(cluster_name_to_id),\n",
    "                    )\n",
    "                ]\n",
    "            ) #.empty_field()\n",
    "            fields[\"span_saliency_labels\"] = SequenceLabelField(\n",
    "                [0], fields[\"spans\"], label_namespace=\"span_saliency_labels\"\n",
    "            )\n",
    "            fields[\"span_type_labels\"] = SequenceLabelField(\n",
    "                [\"Method\"], fields[\"spans\"], label_namespace=\"span_type_labels\"\n",
    "            )\n",
    "            fields[\"span_features\"] = ListField(\n",
    "                [MultiLabelField([], label_namespace=\"section_feature_labels\", num_labels=5)]\n",
    "            )\n",
    "\n",
    "        if len(relation_to_cluster_ids) > 0:\n",
    "            fields[\"relation_to_cluster_ids\"] = ListField(\n",
    "                [\n",
    "                    MultiLabelField(\n",
    "                        v,\n",
    "                        label_namespace=\"cluster_labels\",\n",
    "                        skip_indexing=True,\n",
    "                        num_labels=len(cluster_name_to_id),\n",
    "                    )\n",
    "                    for k, v in relation_to_cluster_ids.items()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f91575a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields.span_field import SpanField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b3d8ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SpanField??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f06f9e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListField of 1 SpanFields : \n",
      " \t SpanField with spans: (-1, -1). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(instance.get('spans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e43b69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95328dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"tokens\": SingleIdTokenIndexer()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aa23945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_dict(json_dict):\n",
    "    # Get fields from JSON dict\n",
    "    entities: List[Tuple[int, int, BaseEntityType]] = json_dict.get(\"ner\", [])\n",
    "    # Convert Entities to dictionary {(s, e) -> type}\n",
    "    entities = sorted(entities, key=lambda x: (x[0], x[1]))\n",
    "    entities: Dict[Span, BaseEntityType] = OrderedDict([((s, e), t) for s, e, t in entities])\n",
    "\n",
    "    clusters_dict: Dict[ClusterName, List[Span]] = {\n",
    "        cluster_name: sorted(list(set([tuple(x) for x in spans])))\n",
    "        for cluster_name, spans in json_dict.get('coref', dict()).items()\n",
    "    }\n",
    "\n",
    "    n_ary_relations: List[Dict[BaseEntityType, ClusterName]] = [x for x in json_dict.get(\"n_ary_relations\", list())]\n",
    "    existing_entities = set([v for relation in n_ary_relations for k, v in relation.items()])\n",
    "\n",
    "    cluster_to_type: Dict[ClusterName, BaseEntityType] = {}\n",
    "    for rel in n_ary_relations:\n",
    "        for k, v in rel.items():\n",
    "            cluster_to_type[v] = k\n",
    "\n",
    "    # Under current model, we do not use method subrelations as separate component\n",
    "    # Therefore, we add each submethod as a separate entity\n",
    "    if \"method_subrelations\" in json_dict:\n",
    "        # Map each method to set containing (all submethod names and the method name itself) .\n",
    "        method_subrelations: Dict[ClusterName, Set[ClusterName]] = {\n",
    "            k: set([k] + [x[1] for x in v]) for k, v in json_dict[\"method_subrelations\"].items()\n",
    "        }\n",
    "\n",
    "        # Add each submethod to cluster_to_type as Method\n",
    "        for method_name, method_sub in method_subrelations.items():\n",
    "            for m in method_sub:\n",
    "                if m in clusters_dict and m != method_name and m not in existing_entities:\n",
    "                    clusters_dict[method_name] += clusters_dict[m]\n",
    "                    clusters_dict[method_name] = sorted(list(set(clusters_dict[method_name])))\n",
    "\n",
    "                    del clusters_dict[m]\n",
    "\n",
    "    for cluster, spans in clusters_dict.items():\n",
    "        for span in spans:\n",
    "            assert span in entities, breakpoint()\n",
    "            if cluster not in cluster_to_type:\n",
    "                continue\n",
    "            entities[span] = cluster_to_type[cluster]\n",
    "\n",
    "    for e in entities:\n",
    "        entities[e]: EntityType = (entities[e], str(any(e in v for v in clusters_dict.values())))\n",
    "\n",
    "    json_dict[\"ner\"]: Dict[Span, BaseEntityType] = entities\n",
    "    json_dict[\"coref\"]: Dict[ClusterName, List[Span]] = clusters_dict\n",
    "\n",
    "    for e in entities:\n",
    "        in_sentences = [\n",
    "            i for i, s in enumerate(json_dict[\"sentences\"]) if is_x_in_y(e, s)\n",
    "        ]  # Check entity lie in one sentence\n",
    "        if len(in_sentences) > 1:\n",
    "            breakpoint()\n",
    "        if len(in_sentences) == 0:\n",
    "            in_sentences = [i for i, s in enumerate(json_dict[\"sentences\"]) if does_overlap(e, s)]\n",
    "            assert sorted(in_sentences) == list(range(min(in_sentences), max(in_sentences) + 1)), breakpoint()\n",
    "            # breakpoint()\n",
    "            in_sentences = sorted(in_sentences)\n",
    "            json_dict[\"sentences\"][in_sentences[0]][1] = json_dict[\"sentences\"][in_sentences[-1]][1]\n",
    "            json_dict[\"sentences\"] = [\n",
    "                s for i, s in enumerate(json_dict[\"sentences\"]) if i not in in_sentences[1:]\n",
    "            ]\n",
    "\n",
    "    json_dict[\"sentences\"]: List[List[Span]] = group_sentences_to_sections(\n",
    "        json_dict[\"sentences\"], json_dict[\"sections\"]\n",
    "    )\n",
    "\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8f45d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_json_dict(json_dict):\n",
    "    sentences: List[List[Span]] = json_dict[\"sentences\"]\n",
    "    sections: List[Span] = json_dict[\"sections\"]\n",
    "    entities: Dict[Span, str] = json_dict[\"ner\"]\n",
    "    corefs: Dict[str, List[Span]] = json_dict[\"coref\"]\n",
    "\n",
    "    assert all(sum(is_x_in_y(e, s) for s in sections) == 1 for e in entities), breakpoint()\n",
    "    assert all(sum(is_x_in_y(e, ss) for s in sentences for ss in s) == 1 for e in entities), breakpoint()\n",
    "    assert all(\n",
    "        (sections[i][0] == sentences[i][0][0] and sections[i][-1] == sentences[i][-1][-1])\n",
    "        for i in range(len(sections))\n",
    "    ), breakpoint()\n",
    "\n",
    "    assert all(x in entities for k, v in corefs.items() for x in v), breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"scirex_full_reader\")\n",
    "class ScirexFullReader(DatasetReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_indexers: Dict[str, TokenIndexer] = None,\n",
    "        max_paragraph_length: int = 300,\n",
    "        lazy: bool = False,\n",
    "        to_scirex_converter: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._token_indexers = token_indexers\n",
    "        self._max_paragraph_length = max_paragraph_length\n",
    "        self.prediction_mode = False\n",
    "\n",
    "        ## Hack so I can reuse same reader to convert scirex\n",
    "        ## format to scierc format\n",
    "        self.to_scierc_converter = to_scirex_converter\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str):\n",
    "        with open(file_path, \"r\") as g:\n",
    "            for _, line in enumerate(g):\n",
    "                json_dict = json.loads(line)\n",
    "                if self.prediction_mode:\n",
    "                    if \"method_subrelations\" in json_dict:\n",
    "                        del json_dict[\"method_subrelations\"]\n",
    "                    json_dict[\"n_ary_relations\"] = []\n",
    "                json_dict = clean_json_dict(json_dict)\n",
    "\n",
    "                verify_json_dict(json_dict)\n",
    "\n",
    "                # Get fields from JSON dict\n",
    "                doc_id = json_dict[\"doc_id\"]\n",
    "                sections: List[Span] = json_dict[\"sections\"]\n",
    "                sentences: List[List[Span]] = json_dict[\"sentences\"]\n",
    "                words: List[str] = json_dict[\"words\"]\n",
    "                entities: Dict[Span, EntityType] = json_dict[\"ner\"]\n",
    "                corefs: Dict[ClusterName, List[Span]] = json_dict[\"coref\"]\n",
    "                n_ary_relations: List[Dict[BaseEntityType, ClusterName]] = json_dict[\"n_ary_relations\"]\n",
    "\n",
    "                # Extract Document structure features\n",
    "                entities_to_features_map: Dict[Span, List[str]] = extract_sentence_features(\n",
    "                    sentences, words, entities\n",
    "                )\n",
    "\n",
    "                # Map cluster names to integer cluster ids\n",
    "                cluster_name_to_id: Dict[ClusterName, int] = {\n",
    "                    k: i for i, k in enumerate(sorted(list(corefs.keys())))\n",
    "                }\n",
    "                max_salient_cluster = len(corefs)\n",
    "\n",
    "                # Map Spans to list of clusters ids it belong to.\n",
    "                span_to_cluster_ids: Dict[Span, List[int]] = {}\n",
    "                for cluster_name in corefs:\n",
    "                    for span in corefs[cluster_name]:\n",
    "                        span_to_cluster_ids.setdefault(span, []).append(cluster_name_to_id[cluster_name])\n",
    "\n",
    "                span_to_cluster_ids = {span: sorted(v) for span, v in span_to_cluster_ids.items()}\n",
    "\n",
    "                assert sorted(list(cluster_name_to_id.values())) == list(\n",
    "                    range(max_salient_cluster)\n",
    "                ), breakpoint()\n",
    "\n",
    "                # Map types to list of cluster ids that are of that type\n",
    "                type_to_cluster_ids: Dict[BaseEntityType, List[int]] = {k: [] for k in used_entities}\n",
    "\n",
    "                for cluster_name in corefs:\n",
    "                    types = [entities[span][0] for span in corefs[cluster_name]]\n",
    "                    if len(set(types)) > 0:\n",
    "                        try :\n",
    "                            type_to_cluster_ids[mode(types)[0][0]].append(cluster_name_to_id[cluster_name])\n",
    "                        except :\n",
    "                            # SciERC gives trouble here. Not relevant .\n",
    "                            continue\n",
    "\n",
    "                # Map relations to list of cluster ids in it.\n",
    "                relation_to_cluster_ids: Dict[int, List[int]] = {}\n",
    "                for rel_idx, rel in enumerate(n_ary_relations):\n",
    "                    relation_to_cluster_ids[rel_idx] = []\n",
    "                    for entity in used_entities:\n",
    "                        relation_to_cluster_ids[rel_idx].append(cluster_name_to_id[rel[entity]])\n",
    "                        type_to_cluster_ids[entity].append(cluster_name_to_id[rel[entity]])\n",
    "\n",
    "                    relation_to_cluster_ids[rel_idx] = tuple(relation_to_cluster_ids[rel_idx])\n",
    "\n",
    "                for k in type_to_cluster_ids:\n",
    "                    type_to_cluster_ids[k] = sorted(list(set(type_to_cluster_ids[k])))\n",
    "\n",
    "                # Move paragraph boundaries around to accomodate in BERT\n",
    "                sections, sentences_grouped, entities_grouped = self.resize_sections_and_group(\n",
    "                    sections, sentences, entities\n",
    "                )\n",
    "\n",
    "                document_metadata = {\n",
    "                    \"cluster_name_to_id\": cluster_name_to_id,\n",
    "                    \"span_to_cluster_ids\": span_to_cluster_ids,\n",
    "                    \"relation_to_cluster_ids\": relation_to_cluster_ids,\n",
    "                    \"type_to_cluster_ids\": type_to_cluster_ids,\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"doc_length\": len(words),\n",
    "                    \"entities_to_features_map\": entities_to_features_map,\n",
    "                }\n",
    "\n",
    "                # Loop over the sections.\n",
    "                for (paragraph_num, ((start_ix, end_ix), sentences, ner_dict)) in enumerate(\n",
    "                    zip(sections, sentences_grouped, entities_grouped)\n",
    "                ):\n",
    "                    paragraph = words[start_ix:end_ix]\n",
    "                    if len(paragraph) == 0:\n",
    "                        breakpoint()\n",
    "\n",
    "                    instance = self.text_to_instance(\n",
    "                        paragraph_num=paragraph_num,\n",
    "                        paragraph=paragraph,\n",
    "                        ner_dict=ner_dict,\n",
    "                        start_ix=start_ix,\n",
    "                        end_ix=end_ix,\n",
    "                        sentence_indices=sentences,\n",
    "                        document_metadata=document_metadata,\n",
    "                    )\n",
    "                    yield instance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8a1e5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_sections_and_group(\n",
    "     sections: List[Span], sentences: List[List[Span]], entities: Dict[Span, EntityType]\n",
    "):\n",
    "    broken_sections = move_boundaries(\n",
    "        break_paragraphs(\n",
    "            collapse_paragraphs(sections, min_len=20, max_len=_max_paragraph_length),\n",
    "            max_len=_max_paragraph_length,\n",
    "        ),\n",
    "        list(entities.keys()),\n",
    "    )\n",
    "\n",
    "    for p, q in zip(broken_sections[:-1], broken_sections[1:]):\n",
    "        if p[1] != q[0] or p[1] < p[0] or q[1] < q[0]:\n",
    "            breakpoint()\n",
    "\n",
    "    sections = broken_sections\n",
    "    entities_grouped = [{} for _ in range(len(sections))]\n",
    "    sentences_grouped = [[] for _ in range(len(sections))]\n",
    "\n",
    "    # Bert is PITA. Group entities into sections they belong to.\n",
    "    for e in entities:\n",
    "        is_in_n_para = 0\n",
    "        for para_id, p in enumerate(sections):\n",
    "            if is_x_in_y(e, p):\n",
    "                entities_grouped[para_id][(e[0], e[1])] = entities[e]\n",
    "                is_in_n_para += 1\n",
    "\n",
    "        assert is_in_n_para == 1, breakpoint()\n",
    "\n",
    "    ## Bert is serious PITA. Need to align sentences with sections also.\n",
    "    sentences = [sent for section in sentences for sent in section]\n",
    "    assert all([sentences[i + 1][0] == sentences[i][1] for i in range(len(sentences) - 1)]), breakpoint()\n",
    "    assert sentences[-1][1] == sections[-1][1], breakpoint()\n",
    "    sentence_indices = sorted(list(set([0] + [s[1] for s in sentences] + [s[1] for s in sections])))\n",
    "    sentences = list(zip(sentence_indices[:-1], sentence_indices[1:]))\n",
    "    for e in sentences:\n",
    "        is_in_n_para = 0\n",
    "        for para_id, p in enumerate(sections):\n",
    "            if is_x_in_y(e, p):\n",
    "                sentences_grouped[para_id].append(e)\n",
    "                is_in_n_para += 1\n",
    "\n",
    "        assert is_in_n_para == 1, breakpoint()\n",
    "\n",
    "    zipped = zip(sections, sentences_grouped, entities_grouped)\n",
    "\n",
    "    # Remove Empty sections\n",
    "    sections, sentences_grouped, entities_grouped = [], [], []\n",
    "    for p, s, e in zipped:\n",
    "        if p[1] - p[0] == 0:\n",
    "            assert len(e) == 0, breakpoint()\n",
    "            assert len(s) == 0, breakpoint()\n",
    "            continue\n",
    "        sections.append(p)\n",
    "        entities_grouped.append(e)\n",
    "        sentences_grouped.append(s)\n",
    "\n",
    "    return sections, sentences_grouped, entities_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8da77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def text_to_instance(\n",
    "        paragraph_num: int,\n",
    "        paragraph: List[str],\n",
    "        ner_dict: Dict[Span, str],\n",
    "        start_ix: int,\n",
    "        end_ix: int,\n",
    "        sentence_indices: List[Span],\n",
    "        document_metadata: Dict[str, Any],\n",
    "    ):\n",
    "\n",
    "        if to_scierc_converter:\n",
    "            return dict(\n",
    "                paragraph_num=paragraph_num,\n",
    "                paragraph=paragraph,\n",
    "                ner_dict=ner_dict,\n",
    "                start_ix=start_ix,\n",
    "                end_ix=end_ix,\n",
    "                sentence_indices=sentence_indices,\n",
    "                document_metadata=document_metadata,\n",
    "            )\n",
    "\n",
    "        text_field = TextField([Token(word) for word in paragraph], _token_indexers)\n",
    "\n",
    "        metadata_field = MetadataField(\n",
    "            dict(\n",
    "                doc_id=document_metadata[\"doc_id\"],\n",
    "                paragraph_num=paragraph_num,\n",
    "                paragraph=paragraph,\n",
    "                start_pos_in_doc=start_ix,\n",
    "                end_pos_in_doc=end_ix,\n",
    "                ner_dict=ner_dict,\n",
    "                sentence_indices=sentence_indices,\n",
    "                document_metadata=document_metadata,\n",
    "                num_spans=len(ner_dict),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ner_type_labels = spans_to_bio_tags(\n",
    "            [(k[0] - start_ix, k[1] - start_ix, v[0]) for k, v in ner_dict.items()], len(paragraph)\n",
    "        )\n",
    "\n",
    "        ner_entity_field = SequenceLabelField(ner_type_labels, text_field, label_namespace=\"ner_type_labels\")\n",
    "\n",
    "        # Pull it  all together.\n",
    "        fields = dict(text=text_field, ner_type_labels=ner_entity_field, metadata=metadata_field)\n",
    "\n",
    "        spans = []\n",
    "        span_cluster_labels = []\n",
    "        span_saliency_labels = []\n",
    "        span_type_labels = []\n",
    "        span_features = []\n",
    "\n",
    "        entities_to_features_map = document_metadata[\"entities_to_features_map\"]\n",
    "        cluster_name_to_id = document_metadata[\"cluster_name_to_id\"]\n",
    "        relation_to_cluster_ids = document_metadata[\"relation_to_cluster_ids\"]\n",
    "        span_to_cluster_ids = document_metadata[\"span_to_cluster_ids\"]\n",
    "\n",
    "        for (s, e), label in ner_dict.items():\n",
    "            spans.append(SpanField(int(s - start_ix), int(e - start_ix - 1), text_field))\n",
    "            span_cluster_labels.append(\n",
    "                MultiLabelField(\n",
    "                    span_to_cluster_ids.get((s, e), []),\n",
    "                    label_namespace=\"cluster_labels\",\n",
    "                    skip_indexing=True,\n",
    "                    num_labels=len(cluster_name_to_id),\n",
    "                )\n",
    "            )\n",
    "            span_saliency_labels.append(1 if label[-1] == \"True\" else 0)\n",
    "            span_type_labels.append(label[0])\n",
    "            span_features.append(\n",
    "                MultiLabelField(entities_to_features_map[(s, e)], label_namespace=\"section_feature_labels\", num_labels=5)\n",
    "            )\n",
    "\n",
    "        if len(spans) > 0:\n",
    "            fields[\"spans\"] = ListField(spans)\n",
    "            fields[\"span_cluster_labels\"] = ListField(span_cluster_labels)\n",
    "            fields[\"span_saliency_labels\"] = SequenceLabelField(\n",
    "                span_saliency_labels, fields[\"spans\"], label_namespace=\"span_saliency_labels\"\n",
    "            )\n",
    "            fields[\"span_type_labels\"] = SequenceLabelField(\n",
    "                span_type_labels, fields[\"spans\"], label_namespace=\"span_type_labels\"\n",
    "            )\n",
    "            fields[\"span_features\"] = ListField(span_features)\n",
    "        else:  # Some paragraphs may not have anything !\n",
    "            fields[\"spans\"] = ListField([SpanField(-1, -1, text_field).empty_field()]).empty_field()\n",
    "            fields[\"span_cluster_labels\"] = ListField(\n",
    "                [\n",
    "                    MultiLabelField(\n",
    "                        [],\n",
    "                        label_namespace=\"cluster_labels\",\n",
    "                        skip_indexing=True,\n",
    "                        num_labels=len(cluster_name_to_id),\n",
    "                    )\n",
    "                ]\n",
    "            ) #.empty_field()\n",
    "            fields[\"span_saliency_labels\"] = SequenceLabelField(\n",
    "                [0], fields[\"spans\"], label_namespace=\"span_saliency_labels\"\n",
    "            )\n",
    "            fields[\"span_type_labels\"] = SequenceLabelField(\n",
    "                [\"Method\"], fields[\"spans\"], label_namespace=\"span_type_labels\"\n",
    "            )\n",
    "            fields[\"span_features\"] = ListField(\n",
    "                [MultiLabelField([], label_namespace=\"section_feature_labels\", num_labels=5)]\n",
    "            )\n",
    "\n",
    "        if len(relation_to_cluster_ids) > 0:\n",
    "            fields[\"relation_to_cluster_ids\"] = ListField(\n",
    "                [\n",
    "                    MultiLabelField(\n",
    "                        v,\n",
    "                        label_namespace=\"cluster_labels\",\n",
    "                        skip_indexing=True,\n",
    "                        num_labels=len(cluster_name_to_id),\n",
    "                    )\n",
    "                    for k, v in relation_to_cluster_ids.items()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e29324a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 12:32:57,096:INFO:Loading Model from \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "file model.tar.gz not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/209030495.tmpdir/ipykernel_50681/1247090785.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/209030495.tmpdir/ipykernel_50681/1247090785.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;31m#argv[3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mcuda_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;31m#int(argv[4])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/209030495.tmpdir/ipykernel_50681/1247090785.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(archive_folder, test_file, output_file, cuda_device)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Model from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0marchive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/allennlp/models/archival.py\u001b[0m in \u001b[0;36mload_archive\u001b[0;34m(archive_file, cuda_device, overrides, weights_file)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/scirex/lib64/python3.7/site-packages/allennlp/common/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# File, but it doesn't exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file {} not found\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# Something unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: file model.tar.gz not found"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import json\n",
    "import os\n",
    "from sys import argv\n",
    "from typing import Dict, List\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from allennlp.common.util import import_submodules\n",
    "from allennlp.data import DataIterator, DatasetReader\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.nn import util as nn_util\n",
    "from allennlp.common.params import Params\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "def predict(archive_folder, test_file, output_file, cuda_device):\n",
    "    '''\n",
    "    test_file contains atleast - doc_id, sections, sentences, ner in scirex format.\n",
    "\n",
    "    output_file - {\n",
    "        'doc_id' : str,\n",
    "        'saliency' : Tuple[start_index, end_index, salient (binary), saliency probability]\n",
    "    }\n",
    "    '''\n",
    "    import_submodules(\"scirex\")\n",
    "    logging.info(\"Loading Model from %s\", archive_folder)\n",
    "    archive_file = os.path.join(archive_folder, \"model.tar.gz\")\n",
    "    archive = load_archive(archive_file, cuda_device)\n",
    "    model = archive.model\n",
    "    model.eval()\n",
    "\n",
    "    saliency_threshold = json.load(open(archive_folder + '/metrics.json'))['best_validation__span_threshold']\n",
    "\n",
    "    model.prediction_mode = True\n",
    "    config = archive.config.duplicate()\n",
    "    dataset_reader_params = config[\"dataset_reader\"]\n",
    "    dataset_reader = DatasetReader.from_params(dataset_reader_params)\n",
    "    dataset_reader.prediction_mode = True\n",
    "    instances = dataset_reader.read(test_file)\n",
    "\n",
    "    for instance in instances :\n",
    "        batch = Batch([instance])\n",
    "        batch.index_instances(model.vocab)\n",
    "    data_iterator = DataIterator.from_params(config[\"validation_iterator\"])\n",
    "    iterator = data_iterator(instances, num_epochs=1, shuffle=False)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        documents = {}\n",
    "        for batch in tqdm(iterator):\n",
    "            batch = nn_util.move_to_device(batch, cuda_device)  # Put on GPU.\n",
    "            output_res = model.decode_saliency(batch, saliency_threshold)\n",
    "            if \"metadata\" not in output_res:\n",
    "                continue\n",
    "            metadata = output_res['metadata']\n",
    "            doc_ids: List[str]= []\n",
    "            for m in metadata:\n",
    "                if \"doc_id\" in m:\n",
    "                    doc_ids.append(m[\"doc_id\"])\n",
    "            if len(doc_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            assert len(set(doc_ids)) == 1\n",
    "            \n",
    "            #doc_ids: List[str] = [m[\"doc_id\"] for m in metadata]\n",
    "            #assert len(set(doc_ids)) == 1\n",
    "\n",
    "            decoded_spans: List[Dict[tuple, float]] = output_res['decoded_spans']\n",
    "            doc_id = metadata[0]['doc_id']\n",
    "\n",
    "            if doc_id not in documents :\n",
    "                documents[doc_id] = {}\n",
    "                documents[doc_id]['saliency'] = []\n",
    "                documents[doc_id]['doc_id'] = doc_id\n",
    "\n",
    "            for pspans in decoded_spans :\n",
    "                for span, prob in pspans.items() :\n",
    "                    documents[doc_id]['saliency'].append([span[0], span[1], 1 if prob > saliency_threshold else 0, prob])\n",
    "\n",
    "        f.write(\"\\n\".join([json.dumps(x) for x in documents.values()]))\n",
    "\n",
    "\n",
    "def main():\n",
    "    archive_folder = \"\"#argv[1]\n",
    "    test_file = \"\"#argv[2]\n",
    "    output_file = \"\"#argv[3]\n",
    "    cuda_device = \"\"#int(argv[4])\n",
    "    predict(archive_folder, test_file, output_file, cuda_device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
