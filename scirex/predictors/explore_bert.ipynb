{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2448bf11",
   "metadata": {},
   "source": [
    "# MLM Training\n",
    "\n",
    "In this notebook we'll cover the training process for masked-language modeling (MLM). First we import and initialize everything required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b88103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45544aa5",
   "metadata": {},
   "source": [
    "We'll be using *Meditations* by *Marcus Aurelius* as our training data. The file below has already been cleaned and so no further processing is required (beyond the `split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7627296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./clean.txt', 'r') as fp:\n",
    "    text = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388904b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From my grandfather Verus I learned good morals and the government of my temper.',\n",
       " 'From the reputation and remembrance of my father, modesty and a manly character.',\n",
       " 'From my mother, piety and beneficence, and abstinence, not only from evil deeds, but even from evil thoughts; and further, simplicity in my way of living, far removed from the habits of the rich.',\n",
       " 'From my great-grandfather, not to have frequented public schools, and to have had good teachers at home, and to know that on such things a man should spend liberally.',\n",
       " \"From my governor, to be neither of the green nor of the blue party at the games in the Circus, nor a partizan either of the Parmularius or the Scutarius at the gladiators' fights; from him too I learned endurance of labour, and to want little, and to work with my own hands, and not to meddle with other people's affairs, and not to be ready to listen to slander.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b02df6",
   "metadata": {},
   "source": [
    "First, we'll tokenize our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05daee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de458102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2013,  2026,  ...,     0,     0,     0],\n",
       "        [  101,  2013,  1996,  ...,     0,     0,     0],\n",
       "        [  101,  2013,  2026,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  3459,  2185,  ...,     0,     0,     0],\n",
       "        [  101,  2043, 15223,  ...,     0,     0,     0],\n",
       "        [  101,  7887,  3288,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a18e03f",
   "metadata": {},
   "source": [
    "Then we create our *labels* tensor by cloning the *input_ids* tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85e79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0599be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f10c8",
   "metadata": {},
   "source": [
    "Now we mask tokens in the *input_ids* tensor, using the 15% probability we used before - and the **not** a *CLS* or *SEP* token condition. This time, because we have padding tokens we also need to exclude *PAD* tokens (*0* input ids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d40e8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00836d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False,  True,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3fa66",
   "metadata": {},
   "source": [
    "And now we take take the indices of each `True` value, within each individual vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e110f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d65b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [4],\n",
       " [19, 24, 29, 34, 36, 39, 41],\n",
       " [22, 27, 28],\n",
       " [2, 4, 6, 12, 13, 17, 18, 27, 28, 37, 44, 48, 51, 52, 55, 64, 67, 87, 89]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea097490",
   "metadata": {},
   "source": [
    "Then apply these indices to each respective row in *input_ids*, assigning each of the values at these indices as *103*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b81e96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca70a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  103, 2026,  ...,    0,    0,    0],\n",
       "        [ 101, 2013, 1996,  ...,    0,    0,    0],\n",
       "        [ 101, 2013, 2026,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3459, 2185,  ...,    0,    0,    0],\n",
       "        [ 101, 2043,  103,  ...,    0,    0,    0],\n",
       "        [ 101, 7887, 3288,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf8e75",
   "metadata": {},
   "source": [
    "We can see the values *103* have been assigned in the same positions as we found *True* values in the `mask_arr` tensor.\n",
    "\n",
    "The `inputs` tensors are now ready, and can we can begin setting them up to be fed into our model during training. We create a PyTorch dataset from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fae9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeditationsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb7a29",
   "metadata": {},
   "source": [
    "Initialize our data using the `MeditationsDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c645d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeditationsDataset(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee80c5b",
   "metadata": {},
   "source": [
    "And initialize the dataloader, which we'll be using to load our data into the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf826850",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf015e",
   "metadata": {},
   "source": [
    "Now we can move onto setting up the training loop. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439fa986",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a0d8a",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a41cbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243891a0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebaafd",
   "metadata": {},
   "source": [
    "Now we can move onto the training loop, we'll train for two epochs (change `epochs` to modify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cba3ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # for our progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad95f87",
   "metadata": {},
   "source": [
    "loop = tqdm(loader, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a66afee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "batch=next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6474d853",
   "metadata": {},
   "source": [
    "optim.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04aca763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pull all tensor batches required for training\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "074ff555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SIZE:  torch.Size([16, 512])\n",
      "EMBEDDINGS SIZE:  torch.Size([16, 512, 768])\n",
      " embedding_output:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " hidden_states:  torch.Size([16, 512, 768])\n",
      " sequence_output:  torch.Size([16, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# process\n",
    "outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c8a78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.0057,  -6.9682,  -6.9149,  ...,  -6.3493,  -6.1319,  -4.3526],\n",
       "        [-12.7728, -12.5942, -12.6098,  ..., -11.3728, -10.7302,  -9.2989],\n",
       "        [ -9.3654,  -8.8331,  -9.0386,  ...,  -8.0218,  -8.8608,  -6.3913],\n",
       "        ...,\n",
       "        [ -9.8041,  -9.6077,  -9.1870,  ...,  -9.2433,  -8.9549,  -4.8263],\n",
       "        [ -6.5214,  -6.5661,  -6.7110,  ...,  -5.4897,  -6.0696,  -4.3684],\n",
       "        [ -6.1707,  -6.1827,  -6.3590,  ...,  -5.1155,  -5.6416,  -3.7265]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f64a9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word = torch.argmax(outputs['logits'][0].squeeze(), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "129d750c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2045,  2024,  2176,  4054, 14863,   103,   103,  1997,  1996,\n",
       "         6020,   103,  2114,  2029, 15223,  2323,  3367,  2022,  7887,  2006,\n",
       "          103,  3457,  1010,  1998,  2043, 15223,  2038,  2102, 11156,  2068,\n",
       "         1010, 15223,  2323,  3367, 13387,   103,  2041,   103,  2360,  2006,\n",
       "         2169,  6686,  2947,   103,  2023,  2245,  2003,  2025,  4072,  1024,\n",
       "         2023, 12102,  2000,  6033,  2591,  2586,  1024,  2023,  2029,   103,\n",
       "         2396,  2183,  2000,  2360,  3310,  2025,  2013,  1996,  2613,   103,\n",
       "         1025,  2005,   103,  2323,  3367,  5136,  2009,  2426,   103,   103,\n",
       "          103,  1997,  2477,  2005,   103,  2158,  2025,  2000,  3713,  2013,\n",
       "         2010,  2613,   103,  1012,   103,  1996,  2959,  2003,  2043, 15223,\n",
       "        21146,   103, 16360,  3217,  6776, 15177,   103,   103,  2005,   103,\n",
       "         1010,  2005,  2023,  2003,  2019,   103,  1997,  1996,   103,  2099,\n",
       "         2112,  2306, 14992,  2108,   103, 27267,   103, 21336,  2000,  1996,\n",
       "         2625, 17164,  1998,  2000,  1996,  2566, 24032,  3468,  2112,  1010,\n",
       "         1996,   103,  1010,  1998,  2000,  2049,   103, 26552,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "044eb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b2be634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are four principal abe'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0][:8], skip_special_tokens=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85b6d6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are four principal abe of the superior against which thou shouldst be constantly on guard, and when thou hast detected them, thou shouldst wipe out say on each occasion thus this thought is not necessary : this tends to destroy social union : this which art going to say comes not from the real ; for shouldst consider it among of things for man not to speak from his real. the fourth is when thou sha reproach thy for, for this is an of ther part within thee beingpowered yielding to the less honourable and to the perishable part, the, and to its pleasures.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5eeb076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. there are four principal abettingments of the superior parts against which thou shouldst be constantly on thy guard, and when thou hast detected them, thou shouldst wipe them out. say on each occasion thus : this thought is not necessary : this tends to destroy social union : this which thou art going to say comes not from the real world ; for thou shouldst consider it among the manyest of things for a man not to speak from his real self. and the fourth is when thou shalt reproach thy self self for it, for this is an example of the purer part within thee being overpowered and yielding to the less honourable and to the perishable part, the good, and to its own pleasures.. wise them. them : the,,, : thou shouldstst, thy thy guard, that when art has detected detected encountered abe have found thourtirti have them them them..... : : about : : :, is not so necessary per the the all per the good good good encountered not should thou thou them. and... : : : : : : thou is is necessary thou : them.. so the part :, thou should thou it it all many many multitude the a union \" thou thou thou thou thou thou thou thou,, is world world good world thou thou thou thou thoultrd we thou it many many the manyestest things a a a a him not dare wise it of the own self self self and the it, and over : andltltltrdrl wise the less very ofr self of good good good, own own mostltlt ther, wise self self self the good good other good giving other wise object own pleasures. self ave thou them. \" thourdltlt rep own self best own life many it it, for it example example per wise wise lesser it of good good good for and example to absolute exampleltru wise the honourable like pure purettonse pure greater, of,, and and, to andbby,, less less lessr less good good good to good good pure own them... but., : : :ltltltltlt and andltbby and less self self it per all good good good it to own example example example entirely the pure greater,,,, good good, own own own. \" : the thy them un. that as art spirit not encountered good pleasures one shouldt thou them them them... :. : : : this thought gr not the'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(top_word, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d777842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 30522])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eacd9ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "709f4baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2045,  2024,  2176,  4054, 14863,   103,   103,  1997,  1996,\n",
       "         6020,   103,  2114,  2029, 15223,  2323,  3367,  2022,  7887,  2006,\n",
       "          103,  3457,  1010,  1998,  2043, 15223,  2038,  2102, 11156,  2068,\n",
       "         1010, 15223,  2323,  3367, 13387,   103,  2041,   103,  2360,  2006,\n",
       "         2169,  6686,  2947,   103,  2023,  2245,  2003,  2025,  4072,  1024,\n",
       "         2023, 12102,  2000,  6033,  2591,  2586,  1024,  2023,  2029,   103,\n",
       "         2396,  2183,  2000,  2360,  3310,  2025,  2013,  1996,  2613,   103,\n",
       "         1025,  2005,   103,  2323,  3367,  5136,  2009,  2426,   103,   103,\n",
       "          103,  1997,  2477,  2005,   103,  2158,  2025,  2000,  3713,  2013,\n",
       "         2010,  2613,   103,  1012,   103,  1996,  2959,  2003,  2043, 15223,\n",
       "        21146,   103, 16360,  3217,  6776, 15177,   103,   103,  2005,   103,\n",
       "         1010,  2005,  2023,  2003,  2019,   103,  1997,  1996,   103,  2099,\n",
       "         2112,  2306, 14992,  2108,   103, 27267,   103, 21336,  2000,  1996,\n",
       "         2625, 17164,  1998,  2000,  1996,  2566, 24032,  3468,  2112,  1010,\n",
       "         1996,   103,  1010,  1998,  2000,  2049,   103, 26552,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc48c364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2045,  2024,  2176,  4054, 14863, 11335,  9285,  1997,  1996,\n",
       "         6020,  4513,  2114,  2029, 15223,  2323,  3367,  2022,  7887,  2006,\n",
       "        15177,  3457,  1010,  1998,  2043, 15223,  2038,  2102, 11156,  2068,\n",
       "         1010, 15223,  2323,  3367, 13387,  2068,  2041,  1998,  2360,  2006,\n",
       "         2169,  6686,  2947,  1024,  2023,  2245,  2003,  2025,  4072,  1024,\n",
       "         2023, 12102,  2000,  6033,  2591,  2586,  1024,  2023,  2029, 15223,\n",
       "         2396,  2183,  2000,  2360,  3310,  2025,  2013,  1996,  2613,  4301,\n",
       "         1025,  2005, 15223,  2323,  3367,  5136,  2009,  2426,  1996,  2087,\n",
       "        18691,  1997,  2477,  2005,  1037,  2158,  2025,  2000,  3713,  2013,\n",
       "         2010,  2613,  4301,  1012,  2021,  1996,  2959,  2003,  2043, 15223,\n",
       "        21146,  7096, 16360,  3217,  6776, 15177, 11246,  2546,  2005,  2505,\n",
       "         1010,  2005,  2023,  2003,  2019,  3350,  1997,  1996,  7746,  2099,\n",
       "         2112,  2306, 14992,  2108,  2058, 27267,  1998, 21336,  2000,  1996,\n",
       "         2625, 17164,  1998,  2000,  1996,  2566, 24032,  3468,  2112,  1010,\n",
       "         1996,  2303,  1010,  1998,  2000,  2049,  7977, 26552,  1012,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73569afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/32 [00:00<?, ?it/s]<ipython-input-14-6dccb434beb5>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:12<00:00,  2.64it/s, loss=0.454]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:11<00:00,  2.86it/s, loss=0.147]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c386dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (4.9.2)\n",
      "Requirement already satisfied: importlib-metadata in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (4.11.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: requests in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: numpy>=1.17 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: packaging in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from packaging->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: joblib in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /cluster/home/fgonzalez/.virtualenvs/scirex/lib64/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/cluster/home/fgonzalez/.virtualenvs/scirex/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5101c",
   "metadata": {},
   "source": [
    "And there we go, we've fine-tuned our BERT model using MLM on *Meditations*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5472f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
